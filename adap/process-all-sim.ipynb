{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f537e8-8027-4d0a-acba-1983ebe304c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, glob, re      # File system, path matching, regular expressions\n",
    "import numpy as np       # Numerical operations\n",
    "import matplotlib as mpl # Main matplotlib library\n",
    "import matplotlib.pyplot as plt # Plotting interface\n",
    "from astropy.io import fits     # FITS file handling\n",
    "from scipy.optimize import minimize # Optimization functions\n",
    "from matplotlib.cm import get_cmap # Colormaps\n",
    "from scipy.signal import find_peaks # Peak finding in signals\n",
    "from matplotlib.backends.backend_pdf import PdfPages # PDF export for plots\n",
    "from collections import defaultdict\n",
    "import cmasher as cmr\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import erf\n",
    "import warnings\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 400      # Set default DPI for figures\n",
    "mpl.rcParams['figure.max_open_warning'] = 0 # Suppress warning for too many open figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f54aad9-9202-436a-8105-2d4eee971ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Globbing Functions ---------- #\n",
    "# Directory containing DU subfolders (du1, du2, du3, ...)\n",
    "ROOT_DIR = \"/Users/leodrake/Library/CloudStorage/Box-Box/IXPE_rmfs/sim_data_mit\" # Adjusted to your path\n",
    "\n",
    "def list_sim_fits(du_specifier='all', pattern='sim_*_pol_recon*.fits'):\n",
    "    \"\"\"\n",
    "    Return a DU label and sorted FITS paths.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    du_specifier : str or list, optional\n",
    "        If 'all', searches all 'du*' subfolders.\n",
    "        If a string like 'du1', searches only that subfolder.\n",
    "        If a list like ['du1', 'du2'], searches those subfolders.\n",
    "        Defaults to 'all'.\n",
    "    pattern : str, optional\n",
    "        The glob pattern for FITS files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    du_label : str\n",
    "        A label representing the processed DUs (e.g., 'du1', 'du_all', 'du_du1_du2').\n",
    "    files : list\n",
    "        A sorted list of found FITS file paths.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(ROOT_DIR):\n",
    "        raise FileNotFoundError(f\"ROOT_DIR '{ROOT_DIR}' not found.\")\n",
    "\n",
    "    all_dus = [d for d in os.listdir(ROOT_DIR) if d.lower().startswith('du') and os.path.isdir(os.path.join(ROOT_DIR, d))]\n",
    "\n",
    "    if isinstance(du_specifier, str) and du_specifier.lower() == 'all':\n",
    "        subdirs_to_search = all_dus\n",
    "        du_label = 'du123'\n",
    "    elif isinstance(du_specifier, str) and du_specifier in all_dus:\n",
    "        subdirs_to_search = [du_specifier]\n",
    "        du_label = du_specifier\n",
    "    elif isinstance(du_specifier, (list, tuple)):\n",
    "        subdirs_to_search = [d for d in du_specifier if d in all_dus]\n",
    "        if not subdirs_to_search:\n",
    "             raise ValueError(f\"None of the specified DUs {du_specifier} found in {ROOT_DIR}\")\n",
    "        du_label = f'du-{\"-\".join(sorted(subdirs_to_search))}'\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid 'du_specifier': {du_specifier}. Use 'all', a valid DU name, or a list.\")\n",
    "\n",
    "    files = []\n",
    "    for sd in subdirs_to_search:\n",
    "        path = os.path.join(ROOT_DIR, sd, pattern)\n",
    "        files.extend(glob.glob(path))\n",
    "\n",
    "    if not files:\n",
    "        print(f\"Warning: No FITS files found for '{du_label}' with pattern '{pattern}' in {ROOT_DIR}.\")\n",
    "\n",
    "    return du_label, sorted(files)\n",
    "\n",
    "def parse_energy(fname):\n",
    "    \"\"\"Extract energy in keV from 'sim_01000_...' filename.\"\"\"\n",
    "    # Search for energy value (5 digits) in filename\n",
    "    m = re.search(r'sim_(\\d{5})', os.path.basename(fname))\n",
    "    # Convert matched digits to float in keV, or return NaN if no match\n",
    "    return int(m.group(1)) / 1000.0 if m else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833378d-aff9-4612-aa0c-d8141469943c",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------- Helper Functions ---------- #\n",
    "\n",
    "def bg_flag(alpha, nrg):\n",
    "    \"\"\"Determine if events are background based on alpha and energy.\"\"\"\n",
    "    # Define parameters for background region boundaries\n",
    "    a1, a2, nrg0 = 0.35, 0.7, 2.0 # Thresholds and reference energy\n",
    "    b1 = (1 - a1) / (5.5 - nrg0)   # Slope for first boundary line\n",
    "    b2 = (0.95 - a2) / (8.0 - nrg0) # Slope for second boundary line\n",
    "    # Identify events above the defined boundary lines as bad\n",
    "    bad = (alpha > (a1 + b1 * (nrg - nrg0))) | (alpha > (a2 + b2 * (nrg - nrg0)))\n",
    "    return ~bad # Return boolean array: True for good events, False for bad\n",
    "\n",
    "# Extract arrays from simulation FITS (full PI range, no XY cut)\n",
    "def extract_common_data(d, pi_min, pi_max, bgflag=False):\n",
    "    \"\"\"\n",
    "    Extract pi, nrg_pi, alpha, and phi from simulation FITS using the raw PHA channel.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : FITS_rec\n",
    "        The table data for one simulation file.\n",
    "    pi_min, pi_max : float\n",
    "        Full-range PI limits in channel units.\n",
    "    bgflag : bool\n",
    "        If True, apply the background cut via bg_flag().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pi, nrg_pi, alpha, phi : tuple of 1D numpy arrays\n",
    "    \"\"\"\n",
    "    # 1) Raw PHA counts (0–32768)\n",
    "    pha = d['PHA'].astype(float) # Extract PHA (Pulse Height Amplitude) column\n",
    "    pha2pi = 1.0 / (3000 * 0.04)  # 3000 ADC counts keV^-1, 40eV bins\n",
    "\n",
    "    # 2) Compute PI channels (0–250) and energy keV (0–10)\n",
    "    pi_raw  = pha * pha2pi # Convert PHA to PI channels\n",
    "    nrg_raw = pha / 3000  # 3000 ADC counts keV^-1: https://doi.org/10.1016/j.astropartphys.2021.102628\n",
    "\n",
    "    # 3) Mask on the full PI range\n",
    "    ok = (pi_raw > pi_min) & (pi_raw <= pi_max) # Create boolean mask for events within PI range\n",
    "\n",
    "    # 4) Slice out the selected events\n",
    "    pi     = pi_raw[ok]       # PI channels for selected events\n",
    "    nrg_pi = nrg_raw[ok]      # Energy (from PI) for selected events\n",
    "    tl     = d['TRK_M2L'][ok] # Track length proxy for selected events\n",
    "    tw     = d['TRK_M2T'][ok] # Track width proxy for selected events\n",
    "    alpha  = (tl - tw) / (tl + tw) # Calculate alpha parameter (shape parameter)\n",
    "    phi    = d['DETPHI2'][ok] # Detector phi angle for selected events\n",
    "\n",
    "    # 5) Optional background filtering\n",
    "    if bgflag:\n",
    "        good = bg_flag(alpha, nrg_pi) # Apply background flag\n",
    "        # Filter data arrays based on good events\n",
    "        pi, nrg_pi, alpha, phi = (\n",
    "            pi[good], nrg_pi[good], alpha[good], phi[good]\n",
    "        )\n",
    "\n",
    "    return pi, nrg_pi, alpha, phi # Return extracted and filtered arrays\n",
    "\n",
    "def find_pi_peak_band(pi, bin_width=1, min_prominence=0.1):\n",
    "    \"\"\"\n",
    "    Identify the *highest* peak in the PI histogram via scipy.signal.find_peaks,\n",
    "    then return its FWHM band.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pi              : 1D array of PI channels\n",
    "    bin_width       : width of each PI bin\n",
    "    min_prominence : minimal prominence for find_peaks (fraction of max count)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pi_min, pi_max : floats\n",
    "        Lower and upper PI channel bounds of the FWHM around the tallest peak.\n",
    "    \"\"\"\n",
    "    # 1) Histogram the PI data\n",
    "    pi = np.asarray(pi) # Ensure PI is a numpy array\n",
    "    lo, hi = int(np.nanmin(pi)), int(np.nanmax(pi)) # Min/max PI values for binning\n",
    "    bins = np.arange(lo, hi + bin_width, bin_width) # Define histogram bins\n",
    "    counts, edges = np.histogram(pi, bins=bins)    # Calculate histogram\n",
    "    centers = (edges[:-1] + edges[1:]) / 2        # Bin centers\n",
    "\n",
    "    # 2) Find *all* peaks with minimal prominence relative to the max count\n",
    "    peak_idxs, props = find_peaks(\n",
    "        counts,\n",
    "        prominence=min_prominence * counts.max()\n",
    "    )\n",
    "    if peak_idxs.size == 0:\n",
    "        # Fallback: if no peaks found, use the highest single bin\n",
    "        idx = counts.argmax()\n",
    "    else:\n",
    "        # Choose the index of the peak with the largest height among found peaks\n",
    "        idx = peak_idxs[np.argmax(counts[peak_idxs])]\n",
    "\n",
    "    # 3) Compute half‐max threshold based on the selected peak's height\n",
    "    half_max = counts[idx] / 2.0\n",
    "\n",
    "    # 4) Walk left/right from the peak to find FWHM boundaries\n",
    "    left = idx # Start search for left FWHM boundary from peak index\n",
    "    while left > 0 and counts[left] > half_max:\n",
    "        left -= 1\n",
    "    right = idx # Start search for right FWHM boundary from peak index\n",
    "    while right < len(counts)-1 and counts[right] > half_max:\n",
    "        right += 1\n",
    "\n",
    "    # 5) Convert FWHM bin indices to PI channel boundaries\n",
    "    pi_min = centers[left] - bin_width/2\n",
    "    pi_max = centers[right] + bin_width/2\n",
    "    return pi_min, pi_max\n",
    "\n",
    "\n",
    "def get_combined_data(file_list):\n",
    "    \"\"\"Reads FITS data from multiple files and concatenates them.\"\"\"\n",
    "    all_data = []\n",
    "    print(f\"    Combining data from {len(file_list)} file(s)...\")\n",
    "    for fname in file_list:\n",
    "        try:\n",
    "            all_data.append(fits.getdata(fname, 1))\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Could not read {os.path.basename(fname)}: {e}\")\n",
    "    if not all_data:\n",
    "        return None\n",
    "    # Ensure all data tables have the same columns before concatenating\n",
    "    if len(all_data) > 1:\n",
    "        first_dtype = all_data[0].dtype\n",
    "        if not all(d.dtype == first_dtype for d in all_data[1:]):\n",
    "            print(\"    Warning: FITS files have different structures. Cannot combine.\")\n",
    "            # Optionally, try to combine common columns or just return the first.\n",
    "            # For now, we'll return None to indicate failure.\n",
    "            return None\n",
    "    return np.concatenate(all_data)\n",
    "\n",
    "# Build step-plot X,Y coordinates for histograms\n",
    "def step_plot(x, y, binwidth):\n",
    "    \"\"\"Create x, y coordinates for a step plot from histogram data.\"\"\"\n",
    "    xsteps, ysteps = [], [] # Initialize lists for step plot coordinates\n",
    "    # For each bin, create two x (left/right edge) and two y (height) points\n",
    "    for xi, yi in zip(x, y):\n",
    "        xsteps += [xi - binwidth/2, xi + binwidth/2]\n",
    "        ysteps += [yi, yi]\n",
    "    return xsteps, ysteps\n",
    "\n",
    "# Plot XY vs Time and φ vs α (Unchanged, but not explicitly used by generate_summary_plots)\n",
    "def plot_xy_vs_t(d, pi_min, pi_max, alpha_min, title):\n",
    "    \"\"\"Generates plots for XY vs Time and phi vs alpha.\"\"\"\n",
    "    # Extract data; bgflag is False by default here\n",
    "    pi, nrg_pi, alpha, phi = extract_common_data(d, pi_min, pi_max, bgflag=False)\n",
    "    time0 = d['TIME'] - np.min(d['TIME']) # Normalize time to start from 0\n",
    "    X, Y = d['ABSX'], d['ABSY'] # Absolute X and Y coordinates\n",
    "    figs = [] # List to store generated figures\n",
    "\n",
    "    # φ histogram\n",
    "    dphi = 0.001 * np.pi # Bin width for phi histogram\n",
    "    phist, edges = np.histogram(phi, bins=np.arange(-np.pi, np.pi + dphi, dphi)) # Compute phi histogram\n",
    "    pval = (edges[:-1] + edges[1:]) / 2 # Phi bin centers\n",
    "    xs, ys = step_plot(pval, phist, dphi) # Get step plot coordinates\n",
    "    fig, ax = plt.subplots(); ax.plot(xs, ys); ax.set(title=f'{title} – φ dist', xlabel='φ'); figs.append(fig)\n",
    "\n",
    "    # φ vs α scatter plot\n",
    "    fig, ax = plt.subplots(); ax.scatter(phi, alpha, s=1) # s=1 for small marker size\n",
    "    ax.set(title=f'{title} – φ vs α', xlabel='φ', ylabel='α'); figs.append(fig)\n",
    "    return figs\n",
    "\n",
    "# Plot PI & α distributions (Unchanged)\n",
    "def plot_pi_alpha(d, pi_min, pi_max, title, dist_component='Full', a_color='blueviolet'):\n",
    "    \"\"\"Generates plots for PI and alpha distributions.\"\"\"\n",
    "    # Extract data; bgflag is False by default here\n",
    "    pi, _, alpha, _ = extract_common_data(d, pi_min, pi_max, bgflag=False)\n",
    "    figs = [] # List to store generated figures\n",
    "\n",
    "    # PI distribution plot\n",
    "    phist, edges = np.histogram(pi, bins=np.arange(pi_min, pi_max+1, 1)) # PI histogram (bin width 1)\n",
    "    pval = (edges[:-1] + edges[1:]) / 2 # PI bin centers\n",
    "    xs, ys = step_plot(pval, phist, 1) # Get step plot coordinates\n",
    "    fig, ax = plt.subplots(); ax.plot(xs, ys, 'k', label=f'{dist_component} distribution')\n",
    "    ax.set(title=f'{title} – PI dist', xlabel='PI'); figs.append(fig)\n",
    "    ax.legend()\n",
    "\n",
    "    # α distribution plot\n",
    "    ahist, edges = np.histogram(alpha, bins=np.arange(0, 1.0, 0.01)) # Alpha histogram (bin width 0.01)\n",
    "    aval = (edges[:-1] + edges[1:]) / 2 # Alpha bin centers\n",
    "    xs, ys = step_plot(aval, ahist, 0.01) # Get step plot coordinates\n",
    "    fig, ax = plt.subplots(); ax.plot(xs, ys, a_color, label=f'{dist_component} distribution')\n",
    "    ax.set(title=f'{title} – α dist', xlabel='α'); figs.append(fig)\n",
    "    ax.legend()\n",
    "    return figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c4adb-711c-4e88-8c86-c8f1a3754a5e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ---------- Fitting Functions ---------- #\n",
    "\n",
    "def minimizer(func, p0, args=(), tol=1e-6):\n",
    "    \"\"\"Wrapper for scipy.optimize.minimize using Nelder-Mead method.\"\"\"\n",
    "    # Perform minimization\n",
    "    res = minimize(func, p0, args=args, method='Nelder-Mead', tol=tol)\n",
    "    return res.x # Return the optimal parameters\n",
    "\n",
    "def safe_polar_likelihood_1d(param, ci):\n",
    "    \"\"\"Calculates -2*log-likelihood for 1D polarization, with penalty for invalid parameters.\"\"\"\n",
    "    # param is modulation factor mu, ci is cos(2*phase_i)\n",
    "    val = 1 + param * ci # Argument of the logarithm in likelihood function\n",
    "    # If any argument is non-positive, return a large penalty value (high cost)\n",
    "    if np.any(val <= 0):\n",
    "        return 1e6 + 1e3 * abs(param)\n",
    "    return -2 * np.sum(np.log(val)) # Standard -2*log-likelihood\n",
    "\n",
    "def invert_matrix(matrix):\n",
    "    \"\"\"Safely inverts a matrix, handling potential LinAlgError.\"\"\"\n",
    "    try:\n",
    "        return 0, np.linalg.inv(matrix) # Return 0 (success) and inverted matrix\n",
    "    except np.linalg.LinAlgError:\n",
    "        return 1, None # Return 1 (error) and None\n",
    "\n",
    "def polar_likelihood(param, evtq, evtu):\n",
    "    \"\"\"Calculates -2*log-likelihood for Stokes Q, U parameters.\"\"\"\n",
    "    q, u = param # Unpack Stokes Q and U from parameters\n",
    "    arg = 1 + q*evtq + u*evtu # Argument of log: 1 + Q*cos(2*phi) + U*sin(2*phi)\n",
    "    # Raise error if any argument is non-positive (log undefined)\n",
    "    if np.any(arg <= 0):\n",
    "        raise ValueError(\"Non-positive argument encountered in log\")\n",
    "    return -2 * np.sum(np.log(arg)) # Sum of -2*log values for all events\n",
    "\n",
    "def polar_evpa_likelihood(param, evtq, evtu):\n",
    "    \"\"\"Calculates -2*log-likelihood for polarization degree (P) and angle (EVPA).\"\"\"\n",
    "    dtor = np.pi/180.0 # Degrees to radians conversion factor\n",
    "    # Convert P, EVPA to Q, U\n",
    "    q = param[0]*np.cos(2*param[1]*dtor) # param[0] is P, param[1] is EVPA\n",
    "    u = param[0]*np.sin(2*param[1]*dtor)\n",
    "    arg = 1 + q*evtq + u*evtu # Argument of log\n",
    "    # Raise error if any argument is non-positive\n",
    "    if np.any(arg <= 0):\n",
    "        raise ValueError(\"Non-positive argument encountered in log\")\n",
    "    return -2 * np.sum(np.log(arg)) # Sum of -2*log values\n",
    "\n",
    "def pderiv(func, x, i, dx):\n",
    "    \"\"\"Computes partial derivative of func w.r.t. x[i] using central difference.\"\"\"\n",
    "    x0, x1 = x.copy(), x.copy() # Create copies of parameter vector\n",
    "    x0[i] -= 0.5*dx # Perturb parameter x[i] backward\n",
    "    x1[i] += 0.5*dx # Perturb parameter x[i] forward\n",
    "    return (func(x1) - func(x0)) / dx # Central difference formula\n",
    "\n",
    "def pderiv2(func, x, dx):\n",
    "    \"\"\"Computes the Hessian matrix (matrix of second partial derivatives) of func.\"\"\"\n",
    "    n = len(x) # Number of parameters\n",
    "    H = np.zeros((n,n)) # Initialize Hessian matrix\n",
    "    # Iterate over upper triangle of the Hessian\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            x0, x1 = x.copy(), x.copy()\n",
    "            # Perturb x[i] to compute derivative w.r.t x[j]\n",
    "            x0[i] -= 0.5*dx[i]; x1[i] += 0.5*dx[i]\n",
    "            # Compute partial derivative of (d func / d x[j]) at x0 and x1\n",
    "            pd0 = pderiv(func, x0, j, dx[j])\n",
    "            pd1 = pderiv(func, x1, j, dx[j])\n",
    "            # Second derivative (d^2 func / dx[i]dx[j])\n",
    "            H[i,j] = (pd1 - pd0)/dx[i]\n",
    "    # Symmetrize the Hessian (H[j,i] = H[i,j])\n",
    "    return H + H.T - np.diag(np.diagonal(H))\n",
    "\n",
    "def likelihood(evtq, evtu):\n",
    "    \"\"\"Estimates Stokes Q, U, polarization degree, EVPA, and errors.\"\"\"\n",
    "    # evtq = cos(2*phi_i), evtu = sin(2*phi_i) for each event i\n",
    "    sumq2 = np.sum(evtq**2) # Sum of cos^2(2*phi_i)\n",
    "    sumu2 = np.sum(evtu**2) # Sum of sin^2(2*phi_i)\n",
    "\n",
    "    # Simple (approximate) initial estimates for Q, U errors\n",
    "    qu_err_init = np.array([1/np.sqrt(sumq2) if sumq2>0 else np.nan,\n",
    "                            1/np.sqrt(sumu2) if sumu2>0 else np.nan])\n",
    "    # Simple initial estimates for Q, U values\n",
    "    qu0 = np.array([np.sum(evtq)*(qu_err_init[0]**2) if sumq2>0 else 0.0,\n",
    "                    np.sum(evtu)*(qu_err_init[1]**2) if sumu2>0 else 0.0])\n",
    "\n",
    "    # Fit Q,U using Nelder-Mead minimization of polar_likelihood\n",
    "    initial_like_val = 0 # Initialize\n",
    "    try:\n",
    "        initial_like_val = polar_likelihood(qu0, evtq, evtu) # Calculate initial likelihood\n",
    "        # Set tolerance for minimizer based on initial likelihood value\n",
    "        tol = abs(0.01 / initial_like_val) if initial_like_val != 0 and (sumq2 + sumu2 > 0) else 1e-6\n",
    "    except ValueError: # Catch potential log error if qu0 is problematic\n",
    "        tol = 1e-6\n",
    "\n",
    "    try:\n",
    "        # Perform minimization to find best-fit Q, U\n",
    "        qu = minimizer(polar_likelihood, qu0, args=(evtq, evtu), tol=tol)\n",
    "    except ValueError: # If minimizer fails due to initial values\n",
    "        qu = qu0.copy() # Use initial estimates\n",
    "    except Exception: # Catch other minimization errors\n",
    "        qu = qu0.copy() # Use initial estimates\n",
    "\n",
    "    # Error estimation from Hessian matrix of the likelihood function\n",
    "    qu_err_fit = qu_err_init.copy() # Default to initial error estimates\n",
    "    try:\n",
    "        # Step sizes for numerical differentiation, use initial errors or small default\n",
    "        hess_dx = qu_err_init if np.all(np.isfinite(qu_err_init)) and np.all(qu_err_init > 0) else np.ones_like(qu_err_init)*1e-3\n",
    "        # Compute Hessian (0.5 factor for -2logL)\n",
    "        H = 0.5 * pderiv2(lambda x: polar_likelihood(x, evtq, evtu), qu, hess_dx)\n",
    "        err_code, M = invert_matrix(H) # Invert Hessian to get covariance matrix\n",
    "        if M is not None: # If inversion successful\n",
    "            qu_err_fit = np.sqrt(np.abs(np.diag(M))) # Errors are sqrt of diagonal elements\n",
    "        else: # If inversion failed\n",
    "            qu_err_fit = np.array([np.nan, np.nan])\n",
    "    except: # Catch any other errors during Hessian calculation/inversion\n",
    "        qu_err_fit = np.array([np.nan, np.nan])\n",
    "\n",
    "    # Calculate polarization degree (poln) and electric vector position angle (evpa)\n",
    "    poln = np.linalg.norm(qu) # Polarization degree: sqrt(Q^2 + U^2)\n",
    "    evpa = 0.5 * np.arctan2(qu[1], qu[0]) * (180.0/np.pi) # EVPA in degrees\n",
    "\n",
    "    # Minimum Detectable Polarization (MDP) estimate\n",
    "    mdp = 4.29/np.sqrt(sumq2+sumu2) if sumq2+sumu2>0 else np.nan\n",
    "    # Return initial Q,U (qu0), their errors (qe0=qu_err_init),\n",
    "    # fitted Q,U (qu), their errors (qe=qu_err_fit),\n",
    "    # polarization degree (poln), its error (nan), EVPA, its error (nan),\n",
    "    # change in likelihood (nan), and MDP.\n",
    "    # Note: Errors for poln and evpa are not calculated here via propagation from Q,U errors.\n",
    "    return qu0, qu_err_init, qu, qu_err_fit, poln, np.nan, evpa, np.nan, np.nan, mdp\n",
    "\n",
    "def fit_mu_alpha(d, pi_min, pi_max, nalpha, title, bgflag=False):\n",
    "    \"\"\"\n",
    "    IDL-style fit_mu_alpha for SIM data, full-PI.\n",
    "    Fits modulation factor mu in bins of alpha.\n",
    "\n",
    "    Returns: (outputs_dict, list_of_figures)\n",
    "    \"\"\"\n",
    "    # 1) Extract common data arrays (pi, energy, alpha, phi)\n",
    "    pi, nrg_pi, alpha, phi = extract_common_data(d, pi_min, pi_max, bgflag)\n",
    "    figs = [] # Initialize list to store figures\n",
    "\n",
    "    # 3) Phi distribution + model (Step 2 from IDL might be implicit or handled elsewhere)\n",
    "    dphi = 0.001 * np.pi # Bin width for phi histogram\n",
    "    phist, edges = np.histogram(phi, bins=np.arange(-np.pi, np.pi + dphi, dphi)) # Compute phi histogram\n",
    "    phival = (edges[:-1] + edges[1:]) / 2 # Phi bin centers\n",
    "    evtq = np.cos(2*phi); evtu = np.sin(2*phi) # Per-event Q and U proxies\n",
    "\n",
    "    # Handle cases with no events in the selected PI band\n",
    "    if len(evtq) == 0 or len(evtu) == 0:\n",
    "        # Prepare an empty output structure\n",
    "        outputs = {\n",
    "            \"mu_noweight\": np.nan, \"mu_noweight_err\": np.nan,\n",
    "            \"alpha_bins\": (np.arange(nalpha)+0.5)*(1.0/float(nalpha)),\n",
    "            \"mu_bins\": np.full(nalpha, np.nan), \"mu_bins_err\": np.full(nalpha, np.nan),\n",
    "            \"nevt_bins\": np.zeros(nalpha, int)\n",
    "        }\n",
    "        # Create an empty placeholder plot for phi distribution\n",
    "        phi_fig, phi_ax = plt.subplots()\n",
    "        phi_ax.set(title=rf\"{title} – $\\phi$ Dist. (No Data)\", xlabel=r\"$\\phi$\")\n",
    "        figs.append(phi_fig)\n",
    "        return outputs, figs\n",
    "\n",
    "    # Calculate overall Q, U, polarization, etc. using the likelihood function\n",
    "    qu0, qe0, qu, qe, mu_nw, mu_nw_err, evpa, evpa_err, dlike, mdp = likelihood(evtq, evtu)\n",
    "\n",
    "    # Model for the phi distribution plot\n",
    "    # Correct normalization: number of φ-bins = len(phist)\n",
    "    nbin_phi_hist = len(phist) # Number of bins in the phi histogram\n",
    "    model = np.zeros_like(phival) # Initialize model array\n",
    "    # Calculate model if there are events and histogram bins\n",
    "    if nbin_phi_hist > 0 and len(phi) > 0:\n",
    "        # Model: N_total * (1 + Q*cos(2*phi_val) + U*sin(2*phi_val)) / N_bins\n",
    "        model = len(phi) * (1 + qu[0]*np.cos(2*phival) + qu[1]*np.sin(2*phival)) / nbin_phi_hist\n",
    "    else: # If no phi values, fill model with NaNs\n",
    "        model.fill(np.nan)\n",
    "\n",
    "    # Plot phi distribution with the fitted model\n",
    "    xs2, ys2 = step_plot(phival, phist, dphi) # Coordinates for step plot of histogram\n",
    "    phi_fig, phi_ax = plt.subplots()\n",
    "    phi_ax.plot(xs2, ys2, 'darkseagreen', alpha=0.75, label=\"Data\") # Plot histogram\n",
    "    phi_ax.plot(phival, model, 'k', label=\"Model\") # Plot model\n",
    "    phi_ax.set(title=rf\"{title} – $\\phi$ Dist.\", xlabel=r\"$\\phi$\")\n",
    "    # phi_ax.legend() # Optional: add legend if desired\n",
    "    figs.append(phi_fig)\n",
    "\n",
    "    # 4) Modulation μ in α-bins via phase histograms\n",
    "    # Calculate phase relative to the fitted EVPA\n",
    "    # Handle qu being all NaNs (e.g., if likelihood fit failed or no data)\n",
    "    phase = (phi - 0.5*np.arctan2(qu[1], qu[0])) % np.pi if not np.all(np.isnan(qu)) else np.full_like(phi, np.nan)\n",
    "    dalpha = 1.0/float(nalpha) # Width of each alpha bin\n",
    "    alpha_centers = (np.arange(nalpha)+0.5)*dalpha # Center of each alpha bin\n",
    "\n",
    "    mu_bins = np.zeros(nalpha)      # Initialize array for mu values in alpha bins\n",
    "    mu_err  = np.full(nalpha, np.nan) # Initialize array for mu errors (with NaNs)\n",
    "    nevt    = np.zeros(nalpha, int) # Initialize array for number of events in alpha bins\n",
    "\n",
    "    MIN_EVENTS = 100 # Minimum number of events required in an alpha bin to perform fit\n",
    "\n",
    "    # Iterate over alpha bins to fit mu\n",
    "    for i, ac in enumerate(alpha_centers):\n",
    "        # Select events within the current alpha bin\n",
    "        sel = np.where((alpha >= ac - dalpha/2) & (alpha < ac + dalpha/2))[0]\n",
    "        nevt[i] = sel.size # Number of events in this bin\n",
    "\n",
    "        # Only fit this bin if enough events and phase is valid\n",
    "        if nevt[i] < MIN_EVENTS or np.all(np.isnan(phase)):\n",
    "            mu_bins[i] = np.nan # Not enough events or invalid phase, set mu to NaN\n",
    "            continue # Skip to next alpha bin\n",
    "\n",
    "        ci = np.cos(2*phase[sel]) # cos(2*aligned_phase) for selected events\n",
    "        # Initial guess for mu parameter for 1D likelihood fit\n",
    "        p0 = np.array([np.sum(ci)/np.sum(ci*ci)]) if np.sum(ci*ci)>0 else np.array([0.0])\n",
    "        # Fit mu using safe_polar_likelihood_1d minimizer\n",
    "        mu_bins[i] = minimizer(safe_polar_likelihood_1d, p0, args=(ci,), tol=1e-6)[0]\n",
    "        # Calculate error for mu_bins[i] from the likelihood curvature (1/sqrt(Fisher_information))\n",
    "        denom = np.sum(ci*ci/((1+mu_bins[i]*ci)**2)) # Denominator for error calculation\n",
    "        mu_err[i] = np.sqrt(1/denom) if denom>0 else np.nan\n",
    "\n",
    "        # Optional: Generate phase-plot for this alpha bin if needed for diagnostics\n",
    "        # (Currently, this plot is generated for every valid bin)\n",
    "        pbin = 0.01*np.pi # Bin width for phase histogram in this alpha bin\n",
    "        ph2, ed2 = np.histogram(phase[sel], bins=np.arange(0,np.pi+pbin,pbin)) # Histogram of aligned phases\n",
    "        pv2 = (ed2[:-1]+ed2[1:])/2 # Phase bin centers\n",
    "        norm = ph2/ph2.sum() if ph2.sum() > 0 else ph2 # Normalized counts (density)\n",
    "        model_ph = 0.01*(1+mu_bins[i]*np.cos(2*pv2)) # Model for phase distribution: (1 + mu*cos(2*phase_val)) * dPhase\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        xs3, ys3 = step_plot(pv2, norm, pbin) # Coordinates for step plot\n",
    "        ax.plot(xs3, ys3, 'darkseagreen', label=\"Data\")\n",
    "        ax.plot(pv2, model_ph, 'k--', label=rf\"$\\mu={mu_bins[i]:.2f}$\") # Plot model with fitted mu\n",
    "        ax.set(title=f\"{title} – {ac-dalpha/2:.1f}<α<{ac+dalpha/2:.1f}\",\n",
    "               xlabel=\"Phase (rel. to EVPA)\", ylabel=\"Normalized Counts\")\n",
    "        ax.legend()\n",
    "        figs.append(fig)\n",
    "\n",
    "    # Store results in a dictionary\n",
    "    outputs = {\n",
    "        \"mu_noweight\": mu_nw,           # Overall modulation factor (no alpha weighting)\n",
    "        \"mu_noweight_err\": mu_nw_err,   # Error for mu_noweight\n",
    "        \"alpha_bins\": alpha_centers,    # Centers of alpha bins\n",
    "        \"mu_bins\": mu_bins,             # Fitted mu in each alpha bin\n",
    "        \"mu_bins_err\": mu_err,          # Error of mu in each alpha bin\n",
    "        \"nevt_bins\": nevt               # Number of events in each alpha bin\n",
    "    }\n",
    "    return outputs, figs # Return results dictionary and list of figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f31fe-9b33-4a59-9b87-2b486a15bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Summary Figures ---------- #\n",
    "import matplotlib.patheffects as pe  # Explicit import for text effects used in stackplot labels\n",
    "\n",
    "def peak_with_erf_tail(E, A_peak, E_break, alpha1, alpha2, C_tail, E_trans, W_trans):\n",
    "    \"\"\"\n",
    "    A broken power-law peak that smoothly transitions to a flat, constant tail.\n",
    "    C_tail: The constant height of the flat tail.\n",
    "    E_trans: The energy where the transition to the tail occurs.\n",
    "    W_trans: The width (speed) of the erf transition.\n",
    "    \"\"\"\n",
    "    # Create the broken power-law core shape\n",
    "    if E_break <= 0:\n",
    "        return np.full_like(E, np.inf)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        term1 = (E / E_break)**(-alpha1)\n",
    "        term2 = (E / E_break)**(-alpha2)\n",
    "        peak = A_peak * (term1 + term2)**(-1)\n",
    "    \n",
    "    # Create the erf-based switch (goes smoothly from 0 to 1)\n",
    "    switch = (1 + erf((E - E_trans) / W_trans)) / 2.0\n",
    "    \n",
    "    # Combine the two parts\n",
    "    return peak * (1 - switch) + C_tail * switch\n",
    "\n",
    "def compute_model(eplot, aa=-0.28, bb=0.2, cc=0.21, dd=1./24.):\n",
    "    \"\"\"\n",
    "    Compute the IDL-like empirical model for modulation factor mu vs. energy.\n",
    "    Model: mu = (1 / [(-aa - bb*E)^-4 + (-cc - dd*E)^-4])^0.25,\n",
    "    handling non-finite values.\n",
    "    \"\"\"\n",
    "    # Calculate the two terms in the denominator\n",
    "    term1 = (-aa - bb * eplot)**(-4)\n",
    "    term2 = (-cc - dd * eplot)**(-4)\n",
    "    # Compute model, suppressing errors for invalid operations (e.g., division by zero)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        model = (1.0 / (term1 + term2))**0.25\n",
    "        model[~np.isfinite(model)] = np.nan # Set non-finite results (e.g. from negative bases) to NaN\n",
    "    return model\n",
    "\n",
    "def generate_summary_sim_plots(energies_list, peak_results_list, low_results_list, du_label, filename=\"summary_plots_sim.pdf\"):\n",
    "    \"\"\"\n",
    "    Create a multipage PDF of summary plots for simulation data using only the Erf Tail model.\n",
    "    \"\"\"\n",
    "    # --- Preliminary Filtering of Input Data ---\\\n",
    "    def result_is_valid(res):\n",
    "        \"\"\"Helper function to check if a result dictionary is valid (contains finite mu_noweight).\"\"\"\n",
    "        return (\n",
    "            res is not None and\n",
    "            (\"mu_noweight\" in res) and\n",
    "            np.isfinite(res[\"mu_noweight\"])\n",
    "        )\n",
    "\n",
    "    valid_indices = [\n",
    "        i for i, (p, l, e) in enumerate(zip(peak_results_list, low_results_list, energies_list))\n",
    "        if result_is_valid(p) and result_is_valid(l) and np.isfinite(e)\n",
    "    ]\n",
    "\n",
    "    if not valid_indices:\n",
    "        print(f\"Error: No valid results found for DU {du_label}. Cannot generate summary plots.\")\n",
    "        return\n",
    "\n",
    "    energies = np.array([energies_list[i] for i in valid_indices])\n",
    "    peak_results = [peak_results_list[i] for i in valid_indices]\n",
    "    low_results  = [low_results_list[i] for i in valid_indices]\n",
    "\n",
    "    if len(energies) == 0:\n",
    "        print(f\"Error: No valid data remain after filtering for DU {du_label}. Cannot generate summary plots.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nGenerating summary plots for {len(energies)} valid simulations ({du_label}) in '{filename}'...\")\n",
    "\n",
    "    # --- Precompute Event Counts and Weighted mu ---\\\n",
    "    tot_hi = np.array([np.sum(res[\"nevt_bins\"]) if \"nevt_bins\" in res and res[\"nevt_bins\"] is not None else 0 for res in peak_results])\n",
    "    tot_lo = np.array([np.sum(res[\"nevt_bins\"]) if \"nevt_bins\" in res and res[\"nevt_bins\"] is not None else 0 for res in low_results])\n",
    "    valid_evt_mask = ((tot_hi + tot_lo) > 0)\n",
    "    mu_noweight_hi = np.array([res[\"mu_noweight\"] for res in peak_results])\n",
    "    mu_noweight_hi_err = np.array([res[\"mu_noweight_err\"] for res in peak_results])\n",
    "    mu_noweight_lo = np.array([res[\"mu_noweight\"] for res in low_results])\n",
    "    mu_noweight_lo_err = np.array([res[\"mu_noweight_err\"] for res in low_results])\n",
    "    mu_noweight_combined = np.full_like(mu_noweight_hi, np.nan)\n",
    "    numerator_combined = np.nan_to_num(tot_hi * mu_noweight_hi) + np.nan_to_num(tot_lo * mu_noweight_lo)\n",
    "    denominator_combined = tot_hi + tot_lo\n",
    "    mu_noweight_combined[valid_evt_mask] = numerator_combined[valid_evt_mask] / denominator_combined[valid_evt_mask]\n",
    "\n",
    "    # --- Model Curves ---\\\n",
    "    eplot = np.linspace(min(energies), max(energies), 500)\n",
    "    mu_model_orig   = compute_model(eplot, aa=-0.28, bb=0.2, cc=0.21, dd=1./24.)\n",
    "    mu_model_better = compute_model(eplot, aa=-0.28, bb=0.2, cc=0.21, dd=1./18.5)\n",
    "\n",
    "    with PdfPages(filename) as pdf:\n",
    "        # --- Pages 1-4: Standard Summary Plots ---\\\n",
    "        fig1, ax1 = plt.subplots();\n",
    "        ax1.errorbar(energies, mu_noweight_hi, yerr=mu_noweight_hi_err, c='deeppink', fmt='.', capsize=3, label=\"Peak PI\")\n",
    "        ax1.errorbar(energies, mu_noweight_lo, yerr=mu_noweight_lo_err, c='cornflowerblue', fmt='s', markersize=3, capsize=3, label=\"Low PI\")\n",
    "        ax1.plot(energies, mu_noweight_combined, 'd', markersize=3, c='blueviolet', label=\"All PI\")\n",
    "        ax1.plot(eplot, mu_model_better, 'k', label=\"Peak Only Model\", zorder=10)\n",
    "        ax1.plot(eplot, mu_model_orig, 'k--', label=\"All PI Model (Di Marco+)\", zorder=9)\n",
    "        ax1.set(xlabel=\"Energy (keV)\", ylabel=r\"$\\mu$\", title=f\"IXPE Simulations {du_label}\"); ax1.legend()\n",
    "        pdf.savefig(fig1); plt.close(fig1)\n",
    "\n",
    "        a_ref, b_ref = 0.05, 0.8; a_line_ref = np.linspace(0,1,200); mu_line_ref = a_ref + b_ref*a_line_ref\n",
    "        norm = plt.Normalize(vmin=np.min(energies), vmax=np.max(energies))\n",
    "        fig2, ax2 = plt.subplots();\n",
    "        for E_val, res_dict in zip(energies, peak_results):\n",
    "            mask = res_dict[\"nevt_bins\"] > 100\n",
    "            ax2.plot(res_dict[\"alpha_bins\"][mask], res_dict[\"mu_bins\"][mask], color=plt.cm.rainbow_r(norm(E_val)), lw=1)\n",
    "        ax2.plot(a_line_ref, mu_line_ref, 'k--', linewidth=3, label=\"Di Marco et al.\")\n",
    "        ax2.set(title=\"Peak PI: μ vs α\", xlabel=\"α\", ylabel=\"μ\"); ax2.legend()\n",
    "        fig2.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=\"rainbow_r\"), ax=ax2, label=\"Energy (keV)\")\n",
    "        plt.tight_layout(); pdf.savefig(fig2); plt.close(fig2)\n",
    "\n",
    "        fig3, ax3 = plt.subplots()\n",
    "        for E_val, res_dict in zip(energies, low_results):\n",
    "            mask = res_dict[\"nevt_bins\"] > 100\n",
    "            ax3.plot(res_dict[\"alpha_bins\"][mask], res_dict[\"mu_bins\"][mask], color=plt.cm.rainbow_r(norm(E_val)), lw=1)\n",
    "        ax3.plot(a_line_ref, mu_line_ref, 'k--', linewidth=3, label=\"Di Marco et al.\")\n",
    "        ax3.set(title=\"Low PI: μ vs α\", xlabel=\"α\", ylabel=\"μ\"); ax3.legend()\n",
    "        fig3.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=\"rainbow_r\"), ax=ax3, label=\"Energy (keV)\")\n",
    "        plt.tight_layout(); pdf.savefig(fig3); plt.close(fig3)\n",
    "        \n",
    "        fig4, ax4 = plt.subplots()\n",
    "        a_model_linear, b_model_linear = 0.05, 0.8\n",
    "        for E_val, res_dict in zip(energies, peak_results):\n",
    "            mask = res_dict[\"nevt_bins\"] > 100\n",
    "            mu_improvement = res_dict[\"mu_bins\"] - (a_model_linear + b_model_linear * res_dict[\"alpha_bins\"])\n",
    "            ax4.plot(res_dict[\"alpha_bins\"][mask], mu_improvement[mask], color=plt.cm.rainbow_r(norm(E_val)), linewidth=1)\n",
    "        ax4.axhline(0, color='k', linestyle='--', linewidth=3)\n",
    "        ax4.set(title=\"Peak PI: Model Improvement\", xlabel=\"α\", ylabel=\"μ − model (linear)\")\n",
    "        # FIX: Use ax=ax4, not ax4=ax4\n",
    "        fig4.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=\"rainbow_r\"), ax=ax4, label=\"Energy (keV)\")\n",
    "        plt.tight_layout(); pdf.savefig(fig4); plt.close(fig4)\n",
    "\n",
    "        # ===================================================================\n",
    "        # --- NEW PLOT: Split Alpha Fraction (Low PI vs Peak PI) (Page 5) --\n",
    "        # ===================================================================\n",
    "        print(\"   -> Generating Split Alpha Fraction plots...\")\n",
    "        \n",
    "        # Helper variables\n",
    "        n_alpha_bins = len(peak_results[0][\"alpha_bins\"])\n",
    "        alpha_bin_centers = peak_results[0][\"alpha_bins\"]\n",
    "        d_alpha = 1.0 / n_alpha_bins\n",
    "        \n",
    "        labels = [fr\"${center-d_alpha/2.0:.2f} \\leq \\alpha < {center+d_alpha/2.0:.2f}$\" for center in alpha_bin_centers]\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, n_alpha_bins)) \n",
    "\n",
    "        # Data preparation: 2 * n_alpha_bins layers (Low PI layers first, then Peak PI layers)\n",
    "        # We will normalize so the total stack height is 1.0\n",
    "        stack_data = [[] for _ in range(2 * n_alpha_bins)]\n",
    "        energies_for_stackplot = []\n",
    "        boundary_line = [] # To draw the line between Low and Peak\n",
    "        \n",
    "        for E_val, res_p, res_l in zip(energies, peak_results, low_results):\n",
    "            n_peak = res_p[\"nevt_bins\"]\n",
    "            n_low  = res_l[\"nevt_bins\"]\n",
    "            \n",
    "            tot_peak = np.sum(n_peak)\n",
    "            tot_low  = np.sum(n_low)\n",
    "            total = tot_peak + tot_low\n",
    "            \n",
    "            if total > 0:\n",
    "                energies_for_stackplot.append(E_val)\n",
    "                \n",
    "                # Fractions of the TOTAL for Low PI (Bottom half)\n",
    "                fracs_low = n_low / total\n",
    "                for i in range(n_alpha_bins):\n",
    "                    stack_data[i].append(fracs_low[i])\n",
    "                    \n",
    "                # Fractions of the TOTAL for Peak PI (Top half)\n",
    "                fracs_peak = n_peak / total\n",
    "                for i in range(n_alpha_bins):\n",
    "                    # stack_data index shifted by n_alpha_bins\n",
    "                    stack_data[n_alpha_bins + i].append(fracs_peak[i])\n",
    "                \n",
    "                # The boundary line is the total fraction of Low PI events\n",
    "                boundary_line.append(tot_low / total)\n",
    "\n",
    "        if energies_for_stackplot:\n",
    "            fig5, ax5 = plt.subplots(figsize=(12, 7))\n",
    "            \n",
    "            # We stack all 2*N layers. \n",
    "            # The first N are Low PI (purple->yellow).\n",
    "            # The next N are Peak PI (purple->yellow), starting where Low PI left off.\n",
    "            # We recycle the 'colors' list so both blocks look like full gradients.\n",
    "            full_colors = list(colors) + list(colors)\n",
    "            \n",
    "            # We only want 1 set of labels for the legend, so we use labels for first N, then None\n",
    "            full_labels = labels + [None] * n_alpha_bins\n",
    "            \n",
    "            ax5.stackplot(energies_for_stackplot, stack_data, labels=full_labels, colors=full_colors)\n",
    "            \n",
    "            # Plot the Boundary Line\n",
    "            ax5.plot(energies_for_stackplot, boundary_line, color='white', linewidth=3, linestyle='-')\n",
    "            ax5.plot(energies_for_stackplot, boundary_line, color='black', linewidth=1.5, linestyle='--')\n",
    "            \n",
    "            # Add Text Annotations to clarify regions using 'pe' for path effects\n",
    "            mid_energy = (min(energies) + max(energies)) / 2\n",
    "            ax5.text(mid_energy, 0.1, \"Low PI\", ha='center', va='center', \n",
    "                     fontsize=12, fontweight='bold', color='white',  path_effects=[pe.withStroke(linewidth=3, foreground=\"black\")])\n",
    "            ax5.text(mid_energy, 0.9, \"Peak PI\", ha='center', va='center', \n",
    "                     fontsize=12, fontweight='bold', color='white', path_effects=[pe.withStroke(linewidth=3, foreground=\"black\")])\n",
    "\n",
    "            ax5.set(title=\"Alpha Fraction Distribution (Split by PI Selection)\", \n",
    "                    xlabel=\"Energy (keV)\", \n",
    "                    ylabel=\"Fraction of Total Events\", \n",
    "                    ylim=(0, 1), \n",
    "                    xlim=(min(energies), max(energies)))\n",
    "            \n",
    "            # Legend (only shows the alpha bins once)\n",
    "            ax5.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), fontsize='small', title=\"Alpha Bins\")\n",
    "            fig5.tight_layout(rect=[0, 0, 0.8, 1])\n",
    "            pdf.savefig(fig5); plt.close(fig5)\n",
    "\n",
    "        # ===================================================================\n",
    "        # --- mu vs. Energy for each alpha bin (UNCHANGED) ---\n",
    "        # ===================================================================\n",
    "        print(\"   -> Generating μ vs. Energy plots for each alpha bin...\")\n",
    "        \n",
    "        erf_tail_params_log = []\n",
    "        erf_tail_params_log_low = []\n",
    "        \n",
    "        last_successful_ebreak_peak = None\n",
    "        last_successful_ebreak_low = None\n",
    "        \n",
    "        for i in range(n_alpha_bins):\n",
    "            alpha_low, alpha_high = alpha_bin_centers[i] - d_alpha/2.0, alpha_bin_centers[i] + d_alpha/2.0\n",
    "            mu_peak_slice = np.array([res['mu_bins'][i] for res in peak_results])\n",
    "            mu_err_peak_slice = np.array([res['mu_bins_err'][i] for res in peak_results])\n",
    "            mu_low_slice = np.array([res['mu_bins'][i] for res in low_results])\n",
    "            mu_err_low_slice = np.array([res['mu_bins_err'][i] for res in low_results])\n",
    "\n",
    "            fig_bin, ax_bin = plt.subplots()\n",
    "            ax_bin.errorbar(energies, mu_peak_slice, yerr=mu_err_peak_slice, c='deeppink', fmt='.', capsize=3, label=\"Peak PI\")\n",
    "            ax_bin.errorbar(energies, mu_low_slice, yerr=mu_err_low_slice, c='cornflowerblue', fmt='s', markersize=3, capsize=3, label=\"Low PI\")\n",
    "            \n",
    "            e_fit_plot = np.linspace(energies.min(), energies.max(), 200)\n",
    "            erf_tail_params_this_bin = [np.nan] * 7\n",
    "            erf_tail_params_this_bin_low = [np.nan] * 7\n",
    "\n",
    "            # Fit Peak PI data\n",
    "            mask_peak = np.isfinite(mu_peak_slice) & (mu_err_peak_slice > 0)\n",
    "            if np.sum(mask_peak) > 6:\n",
    "                x_peak, y_peak, yerr_peak = energies[mask_peak], mu_peak_slice[mask_peak], mu_err_peak_slice[mask_peak]\n",
    "                try:\n",
    "                    p0_erf = [2 * alpha_high, 2, 6, -2.0, 0.1, 6.0, 2.0]\n",
    "                    \n",
    "                    lower_bounds = [2 * alpha_high - 0.001, 1.5, 5, -np.inf, 0, 4, 0.1]\n",
    "                    upper_bounds = [2 * alpha_high + 0.001, 6, np.inf, 0, 1, 12, 10]\n",
    "                    \n",
    "                    if last_successful_ebreak_peak is not None:\n",
    "                        lower_bounds[1] = last_successful_ebreak_peak\n",
    "                        p0_erf[1] = last_successful_ebreak_peak \n",
    "                    \n",
    "                    bounds_erf = (lower_bounds, upper_bounds)\n",
    "                    popt_erf, _ = curve_fit(peak_with_erf_tail, x_peak, y_peak, p0=p0_erf, sigma=yerr_peak, maxfev=10000, bounds=bounds_erf)\n",
    "                    \n",
    "                    ax_bin.plot(e_fit_plot, peak_with_erf_tail(e_fit_plot, *popt_erf), color='k', ls='-', lw=2, label='Erf Tail (Peak PI)')\n",
    "                    erf_tail_params_this_bin = popt_erf\n",
    "                    last_successful_ebreak_peak = popt_erf[1] \n",
    "                except (RuntimeError, ValueError):\n",
    "                    pass\n",
    "            \n",
    "            erf_tail_params_log.append(erf_tail_params_this_bin)\n",
    "\n",
    "            # Fit Low PI data\n",
    "            mask_low = np.isfinite(mu_low_slice) & (mu_err_low_slice > 0)\n",
    "            if np.sum(mask_low) > 4:\n",
    "                x_low, y_low, yerr_low = energies[mask_low], mu_low_slice[mask_low], mu_err_low_slice[mask_low]\n",
    "                try:\n",
    "                    p0_erf_low = [1, 3, 2.0, -2.0, 0.1, 6.0, 0.5]\n",
    "\n",
    "                    lower_bounds_low = [0, 2, 0, -np.inf, 0, 4, 0.1]\n",
    "                    upper_bounds_low = [2, 6, np.inf, 0, np.inf, 12, 1]\n",
    "                    \n",
    "                    if last_successful_ebreak_low is not None:\n",
    "                        lower_bounds_low[1] = last_successful_ebreak_low\n",
    "                        p0_erf_low[1] = last_successful_ebreak_low \n",
    "                    \n",
    "                    bounds_erf_low = (lower_bounds_low, upper_bounds_low)\n",
    "                    popt_erf_low, _ = curve_fit(peak_with_erf_tail, x_low, y_low, p0=p0_erf_low, sigma=yerr_low, maxfev=10000, bounds=bounds_erf_low)\n",
    "                    \n",
    "                    ax_bin.plot(e_fit_plot, peak_with_erf_tail(e_fit_plot, *popt_erf_low), color='k', ls='--', lw=1.5, label='Erf Tail (Low PI)')\n",
    "                    erf_tail_params_this_bin_low = popt_erf_low\n",
    "                    last_successful_ebreak_low = popt_erf_low[1]\n",
    "                except (RuntimeError, ValueError):\n",
    "                    pass\n",
    "            \n",
    "            erf_tail_params_log_low.append(erf_tail_params_this_bin_low)\n",
    "\n",
    "            ax_bin.set(xlabel=\"Energy (keV)\", ylabel=r\"$\\mu$\", title=fr\"Modulation vs. Energy for ${alpha_low:.2f} \\leq \\alpha < {alpha_high:.2f}$\")\n",
    "            ax_bin.legend(); ax_bin.grid(True, linestyle='--', alpha=0.6); ax_bin.set_ylim(-0.15, 1.05)\n",
    "            pdf.savefig(fig_bin); plt.close(fig_bin)\n",
    "\n",
    "        # ===================================================================\n",
    "        # --- Final Parameter Summary Plots (UNCHANGED) ---\n",
    "        # ===================================================================\n",
    "        print(\"   -> Generating parameter vs. alpha plots...\")\n",
    "        erf_param_names = ['Peak Amp (A_peak)', 'Break Energy (E_break)', 'Rise Index (α1)', 'Fall Index (α2)', 'Tail Height (C_tail)', 'Transition Energy (E_trans)', 'Transition Width (W_trans)']\n",
    "        \n",
    "        # Plot for Peak PI parameters\n",
    "        erf_params = np.array(erf_tail_params_log)\n",
    "        fig_erf, axes_erf = plt.subplots(4, 2, figsize=(12, 16), constrained_layout=True)\n",
    "        fig_erf.suptitle('Peak + Erf Tail Fit Parameters vs. α (Peak PI)', fontsize=16)\n",
    "        if len(erf_param_names) % 2 != 0: axes_erf.flat[-1].set_visible(False)\n",
    "        for idx, ax in enumerate(axes_erf.flat):\n",
    "            if idx < len(erf_param_names):\n",
    "                ax.plot(alpha_bin_centers, erf_params[:, idx], 'o-', color='deeppink')\n",
    "                ax.set(title=erf_param_names[idx], xlabel='α bin center', ylabel='Parameter Value'); ax.grid(True, ls=':')\n",
    "        pdf.savefig(fig_erf); plt.close(fig_erf)\n",
    "\n",
    "        # Plot for Low PI parameters\n",
    "        erf_params_low = np.array(erf_tail_params_log_low)\n",
    "        fig_erf_low, axes_erf_low = plt.subplots(4, 2, figsize=(12, 16), constrained_layout=True)\n",
    "        fig_erf_low.suptitle('Peak + Erf Tail Fit Parameters vs. α (Low PI)', fontsize=16)\n",
    "        if len(erf_param_names) % 2 != 0: axes_erf_low.flat[-1].set_visible(False)\n",
    "        for idx, ax in enumerate(axes_erf_low.flat):\n",
    "            if idx < len(erf_param_names):\n",
    "                ax.plot(alpha_bin_centers, erf_params_low[:, idx], 'o-', color='cornflowerblue')\n",
    "                ax.set(title=erf_param_names[idx], xlabel='α bin center', ylabel='Parameter Value'); ax.grid(True, ls=':')\n",
    "        pdf.savefig(fig_erf_low); plt.close(fig_erf_low)\n",
    "\n",
    "    print(f\"Summary plots saved to '{filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3dd5b-1239-4eee-8c03-674dbfacfc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DEFINE YOUR DATASETS\n",
    "#    Maps a short label to the actual subdirectory name.\n",
    "DATASET_CONFIG = {\n",
    "    \"original\": \"sim_data_mit\",\n",
    "    \"scrambled_80\": \"scrambled_sim_data_80percent\",\n",
    "    \"scrambled_50\": \"scrambled_sim_data_50percent\"\n",
    "}\n",
    "BASE_PATH = \"/Users/leodrake/Library/CloudStorage/Box-Box/IXPE_rmfs\"\n",
    "\n",
    "\n",
    "# 2. CHOOSE WHICH DATASET TO PROCESS FOR THIS RUN\n",
    "#    <<<<< CHANGE THIS VALUE to \"original\", \"scrambled_80\", etc. >>>>>\n",
    "CURRENT_DATASET_KEY = \"original\"\n",
    "\n",
    "\n",
    "# 3. DERIVE PATHS AND LABELS (This part is automatic)\n",
    "try:\n",
    "    INPUT_SUBDIR = DATASET_CONFIG[CURRENT_DATASET_KEY]\n",
    "    ROOT_DIR = os.path.join(BASE_PATH, INPUT_SUBDIR)\n",
    "    \n",
    "    # Create a filename suffix ONLY if the dataset is not the original one\n",
    "    if CURRENT_DATASET_KEY == 'original':\n",
    "        output_suffix = ''  # The suffix is empty for the original dataset\n",
    "    else:\n",
    "        output_suffix = f'-{CURRENT_DATASET_KEY}' # For all others, it's '-key'\n",
    "\n",
    "    print(f\" Processing dataset: '{CURRENT_DATASET_KEY}'\")\n",
    "    print(f\"   - Input Directory (ROOT_DIR): '{ROOT_DIR}'\")\n",
    "    print(f\"   - Output filename suffix: '{output_suffix}'\")\n",
    "except KeyError:\n",
    "    raise KeyError(f\"Dataset key '{CURRENT_DATASET_KEY}' not found in DATASET_CONFIG. Available keys: {list(DATASET_CONFIG.keys())}\")\n",
    "\n",
    "\n",
    "# 4. OTHER SETTINGS\n",
    "du_specifier = 'all'\n",
    "force_recompute = False\n",
    "generate_individual_plots = True\n",
    "\n",
    "# ===================================================================\n",
    "# --- END OF CONFIGURATION ---\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5ca2d-06f6-458f-8863-a78c11e4e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the root directory for FITS files exists\n",
    "if not os.path.isdir(ROOT_DIR):\n",
    "    print(f\"Error: ROOT_DIR '{ROOT_DIR}' not found. Please check the path.\")\n",
    "else:\n",
    "    print(f\"--- Processing {du_specifier} ---\")\n",
    "    if force_recompute:\n",
    "        print(\"--- Caching is OFF (force_recompute = True) ---\")\n",
    "\n",
    "    # Set the current working directory (adjust if this path is specific to your setup)\n",
    "    try:\n",
    "        os.chdir('/Users/leodrake/Documents/MIT/IXPE') # adjust if needed\n",
    "        print(f\"Working directory set to: {os.getcwd()}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Could not change directory to '/Users/leodrake/Documents/MIT/IXPE'. Using current: {os.getcwd()}\")\n",
    "\n",
    "\n",
    "    # Get list of FITS files and the DU label\n",
    "    du_label, fits_files = list_sim_fits(du_specifier=du_specifier)\n",
    "\n",
    "    if not fits_files:\n",
    "        print(f\"No FITS files found for {du_label} in {ROOT_DIR}. Exiting.\")\n",
    "    else:\n",
    "        # Group files by energy\n",
    "        files_by_energy = defaultdict(list)\n",
    "        for fname in fits_files:\n",
    "            energy = parse_energy(fname)\n",
    "            if not np.isnan(energy):\n",
    "                files_by_energy[energy].append(fname)\n",
    "        \n",
    "        all_energies_found = sorted(files_by_energy.keys())\n",
    "        \n",
    "        #energies_to_process = [e for e in all_energies_found if (e >= 2.0 and e < 9.0)]\n",
    "        energies_to_process = [e for e in all_energies_found if e <= 8.0]\n",
    "        #energies_to_process = all_energies_found\n",
    "        \n",
    "        print(f\"Found {len(fits_files)} files across {len(energies_to_process)} energy points for '{du_label}'.\")\n",
    "        print(f\"--> Processing {len(energies_to_process)} energy points with E < 9.0 keV.\")\n",
    "\n",
    "        # Create a directory for caching fit results\n",
    "        cache_dir = f'NewRMFsADP/fit-cache-{du_label}{output_suffix}'\n",
    "        os.makedirs(cache_dir, exist_ok=True) # Create if it doesn't exist\n",
    "\n",
    "        # Lists to store results from all files for this DU, for summary plots\n",
    "        all_energies_for_du = []\n",
    "        all_peak_results_for_du = []\n",
    "        all_low_results_for_du = []\n",
    "\n",
    "        # Define output PDF filename for individual file plots\n",
    "        output_pdf_individual = f'process-all-sim-{du_label}{output_suffix}.pdf'\n",
    "        pdf = None # Initialize pdf object to None\n",
    "        \n",
    "        if generate_individual_plots:\n",
    "            print(f\"Individual file plots will be saved to: {os.path.join(os.getcwd(), output_pdf_individual)}\")\n",
    "            pdf = PdfPages(output_pdf_individual) # Open PDF only if needed\n",
    "\n",
    "        # Loop through each energy point\n",
    "        for EkeV_current_file in energies_to_process:\n",
    "            base_name = f\"sim-{EkeV_current_file*1000:05.0f}-{du_label}\"\n",
    "            cache_path_peak = os.path.join(cache_dir, f\"{base_name}-peak.npz\")\n",
    "            cache_path_low = os.path.join(cache_dir, f\"{base_name}-low.npz\")\n",
    "\n",
    "            try:\n",
    "                # --- Caching Check ---\n",
    "                if os.path.exists(cache_path_peak) and os.path.exists(cache_path_low) and not force_recompute:\n",
    "                    print(f\"Loading {EkeV_current_file:.2f} keV from cache...\")\n",
    "                    # Load data directly into the lists\n",
    "                    with np.load(cache_path_peak, allow_pickle=True) as data:\n",
    "                        # Reconstruct the dictionary, handling 0-d arrays if needed\n",
    "                        outputs_peak = {key: data[key].item() if data[key].ndim == 0 else data[key] for key in data}\n",
    "                    with np.load(cache_path_low, allow_pickle=True) as data:\n",
    "                        outputs_low = {key: data[key].item() if data[key].ndim == 0 else data[key] for key in data}\n",
    "                    \n",
    "                    all_energies_for_du.append(EkeV_current_file)\n",
    "                    all_peak_results_for_du.append(outputs_peak)\n",
    "                    all_low_results_for_du.append(outputs_low)\n",
    "                    continue # Skip to the next energy point\n",
    "\n",
    "                # --- Computation (if not cached or forced) ---\n",
    "                print(f\"Processing {EkeV_current_file:.2f} keV...\")\n",
    "                \n",
    "                # Get combined data for this energy\n",
    "                current_files = files_by_energy[EkeV_current_file]\n",
    "                d = get_combined_data(current_files)\n",
    "                if d is None:\n",
    "                    print(f\"    Warning: No data could be loaded for {EkeV_current_file:.2f} keV. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Extract PI data over a broad range (1-374) to find the main peak band\n",
    "                pi_full, _, _, _ = extract_common_data(d, 1, 374, bgflag=False)\n",
    "                if len(pi_full) == 0:\n",
    "                    print(f\"    Warning: No data in full PI range (1-374) for {EkeV_current_file:.2f} keV. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Find PI peak band (e.g., FWHM around the main photopeak)\n",
    "                peak_lo, peak_hi = find_pi_peak_band(pi_full, bin_width=1, min_prominence=0.1)\n",
    "                non_lo, non_hi   = 1, peak_lo # From PI channel 1 up to the start of the peak band\n",
    "                print(f\"    PI Bands - Peak: {peak_lo:.1f}-{peak_hi:.1f}, Low: {non_lo:.1f}-{non_hi:.1f}\")\n",
    "\n",
    "                base_title = f\"SIM {EkeV_current_file:.2f} keV ({du_label}) \" # Updated title\n",
    "                title_non = f\"{base_title}– PI {int(non_lo)}–{int(non_hi)}\"\n",
    "                title_pk = f\"{base_title}– PI {int(peak_lo)}–{int(peak_hi)}\"\n",
    "\n",
    "                # Fit mu vs. alpha for Peak PI band\n",
    "                outputs_peak, fit_figs_peak = fit_mu_alpha(d, peak_lo, peak_hi, nalpha=10, title=title_pk)\n",
    "                # Fit mu vs. alpha for Low PI band\n",
    "                outputs_low, fit_figs_low = fit_mu_alpha(d, non_lo, non_hi, nalpha=10, title=title_non)\n",
    "\n",
    "                # --- Store & Cache Results ---\n",
    "                all_energies_for_du.append(EkeV_current_file)\n",
    "                all_peak_results_for_du.append(outputs_peak)\n",
    "                all_low_results_for_du.append(outputs_low)\n",
    "                np.savez_compressed(cache_path_peak, **outputs_peak)\n",
    "                np.savez_compressed(cache_path_low, **outputs_low)\n",
    "                print(f\"    Results saved to cache for {EkeV_current_file:.2f} keV.\")\n",
    "\n",
    "                # --- Save Individual Plots (if enabled) ---\n",
    "                if pdf: # Check if PDF was opened\n",
    "                    # Plot Full PI (1-374) and its alpha distribution\n",
    "                    figs_full = plot_pi_alpha(d, 1, 374, base_title + '- PI 1-374')\n",
    "                    for fig in figs_full or []: pdf.savefig(fig); plt.close(fig)\n",
    "\n",
    "                    # Plot α distribution of Non-peak (Low PI) region\n",
    "                    figs_non = plot_pi_alpha(d, non_lo, non_hi, title_non, dist_component='Non-Peak', a_color='cornflowerblue')\n",
    "                    if figs_non and len(figs_non) > 1: pdf.savefig(figs_non[1]);\n",
    "                    for fig in figs_non or []: plt.close(fig) # Close all\n",
    "\n",
    "                    # Plot α distribution of Peak PI region\n",
    "                    figs_peak = plot_pi_alpha(d, peak_lo, peak_hi, title_pk, dist_component='Peak', a_color='deeppink')\n",
    "                    if figs_peak and len(figs_peak) > 1: pdf.savefig(figs_peak[1]);\n",
    "                    for fig in figs_peak or []: plt.close(fig) # Close all\n",
    "\n",
    "                    # Save figures from fit_mu_alpha (phi and phase plots)\n",
    "                    for fig in fit_figs_peak or []: pdf.savefig(fig); plt.close(fig)\n",
    "                    for fig in fit_figs_low or []: plt.close(fig) # Close low-PI figs, don't save\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR processing energy {EkeV_current_file:.2f} keV: {e}\")\n",
    "                # Optional: Add cleanup if an error occurs mid-processing for an energy\n",
    "\n",
    "        # Close the individual PDF if it was opened\n",
    "        if pdf:\n",
    "            pdf.close()\n",
    "            print(f\"Individual PDF generation complete.\")\n",
    "\n",
    "        # --- Generate Summary Plots ---\n",
    "        if all_energies_for_du and \\\n",
    "           len(all_energies_for_du) == len(all_peak_results_for_du) and \\\n",
    "           len(all_energies_for_du) == len(all_low_results_for_du):\n",
    "            summary_filename = f'/Users/leodrake/MIT Dropbox/Leonardo Drake/IXPE/simulation-summary-plots-{du_label}{output_suffix}.pdf'\n",
    "            generate_summary_sim_plots(all_energies_for_du, all_peak_results_for_du, all_low_results_for_du,\n",
    "                                       du_label, filename=summary_filename)\n",
    "        else:\n",
    "            print(f\"Data collection issue or no data processed for DU {du_label} \"\n",
    "                  f\"(lengths: E={len(all_energies_for_du)}, P={len(all_peak_results_for_du)}, L={len(all_low_results_for_du)}). \"\n",
    "                  f\"Skipping summary plot generation.\")\n",
    "\n",
    "    print(f\"--- Processing for {du_specifier} complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc2a3da-00e3-41f2-938d-57d89f4866c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import erf\n",
    "\n",
    "# Ensure the main processing cell (Cell 7) has been run first, so these variables exist:\n",
    "# all_energies_for_du, all_peak_results_for_du, all_low_results_for_du\n",
    "\n",
    "# =================================================================================\n",
    "# --- 1. CONFIGURATION: CHOOSE ALPHA BIN ---\n",
    "# =================================================================================\n",
    "# Enter the index of the alpha bin you want to analyze (e.g., 0-9).\n",
    "alpha_bin_index = 6\n",
    "\n",
    "# =================================================================================\n",
    "# --- 2. CUSTOM FIT CONDITIONS ---\n",
    "# Define your experimental conditions for the blue-colored fit.\n",
    "# =================================================================================\n",
    "# Parameters: [A_peak, E_break, alpha1, alpha2, C_tail, E_trans, W_trans]\n",
    "# Indices:    [   0  ,    1   ,    2  ,    3  ,    4   ,     5   ,     6   ]\n",
    "\n",
    "# --- For Peak PI Data ---\n",
    "p0_peak_custom = [1.4, 3, 6, -2.0, 0.113, 6.0, 8]\n",
    "bounds_peak_custom = ([0, 1.5, 5, -np.inf, 0, 0, 0.1], \n",
    "                      [2, 6, np.inf, 0, 1, np.inf, np.inf])\n",
    "# List of parameter indices to FREEZE at their p0 value (e.g., [1, 2] freezes E_break and alpha1)\n",
    "params_to_freeze_peak = [0,1]\n",
    "\n",
    "\n",
    "# --- For Low PI Data ---\n",
    "p0_low_custom = [0.5, 4.0, 3.0, -2.0, 0.1, 7.0, 2.0]\n",
    "bounds_low_custom = ([0, 1.0, 0, -np.inf, 0, 4, 0.1],\n",
    "                     [1, 8, 5, 0, 1, 12, 5.0])\n",
    "# List of parameter indices to FREEZE at their p0 value\n",
    "params_to_freeze_low = [] # Empty list means all parameters are free to vary\n",
    "\n",
    "\n",
    "# =================================================================================\n",
    "\n",
    "\n",
    "# --- Data Preparation ---\n",
    "valid_indices = [\n",
    "    i for i, (p, l, e) in enumerate(zip(all_peak_results_for_du, all_low_results_for_du, all_energies_for_du))\n",
    "    if p is not None and l is not None and np.isfinite(e)\n",
    "]\n",
    "energies = np.array([all_energies_for_du[i] for i in valid_indices])\n",
    "peak_results = [all_peak_results_for_du[i] for i in valid_indices]\n",
    "low_results  = [all_low_results_for_du[i] for i in valid_indices]\n",
    "\n",
    "# --- Extract Data for the Chosen Alpha Bin ---\n",
    "alpha_bin_centers = peak_results[0][\"alpha_bins\"]\n",
    "d_alpha = 1.0 / len(alpha_bin_centers)\n",
    "alpha_center = alpha_bin_centers[alpha_bin_index]\n",
    "alpha_low, alpha_high = alpha_center - d_alpha/2.0, alpha_center + d_alpha/2.0\n",
    "print(f\"--- Fitting Alpha Bin #{alpha_bin_index} ({alpha_low:.2f} <= alpha < {alpha_high:.2f}) ---\")\n",
    "\n",
    "mu_peak_slice = np.array([res['mu_bins'][alpha_bin_index] for res in peak_results])\n",
    "mu_err_peak_slice = np.array([res['mu_bins_err'][alpha_bin_index] for res in peak_results])\n",
    "mu_low_slice = np.array([res['mu_bins'][alpha_bin_index] for res in low_results])\n",
    "mu_err_low_slice = np.array([res['mu_bins_err'][alpha_bin_index] for res in low_results])\n",
    "\n",
    "# --- Perform and Plot Fits ---\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "e_fit_plot = np.linspace(energies.min(), energies.max(), 200)\n",
    "param_names = ['A_peak', 'E_break', 'alpha1', 'alpha2', 'C_tail', 'E_trans', 'W_trans']\n",
    "\n",
    "def calculate_and_print_results(title, y_data, x_data, err_data, popt, frozen_indices=[]):\n",
    "    residuals = y_data - peak_with_erf_tail(x_data, *popt)\n",
    "    chi2 = np.sum((residuals / err_data)**2)\n",
    "    dof = len(x_data) - (len(popt) - len(frozen_indices)) # DoF depends on free parameters\n",
    "    reduced_chi2 = chi2 / dof if dof > 0 else np.inf\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "    print(f\"   - Reduced Chi-Squared: {reduced_chi2:.3f} (Chi^2: {chi2:.2f}, DoF: {dof})\")\n",
    "    for i, (name, val) in enumerate(zip(param_names, popt)):\n",
    "        status = \"(frozen)\" if i in frozen_indices else \"\"\n",
    "        print(f\"   - {name:<10}: {val:.3f} {status}\")\n",
    "\n",
    "# --- Peak PI Fits ---\n",
    "mask_peak = np.isfinite(mu_peak_slice) & (mu_err_peak_slice > 0)\n",
    "if np.sum(mask_peak) > 6:\n",
    "    x_peak, y_peak, yerr_peak = energies[mask_peak], mu_peak_slice[mask_peak], mu_err_peak_slice[mask_peak]\n",
    "    \n",
    "    # 1. Original Fit (all parameters free)\n",
    "    try:\n",
    "        p0_peak_orig = [1, 2, 6, -2.0, 0.1, 6.0, 2.0]\n",
    "        bounds_peak_orig = ([0, 1.5, 5, -np.inf, 0, 4, 0.1], [2, 6, np.inf, 0, 1, 12, np.inf])\n",
    "        popt_peak_orig, _ = curve_fit(peak_with_erf_tail, x_peak, y_peak, p0=p0_peak_orig, sigma=yerr_peak, maxfev=10000, bounds=bounds_peak_orig)\n",
    "        ax.plot(e_fit_plot, peak_with_erf_tail(e_fit_plot, *popt_peak_orig), color='k', ls='-', lw=2, label='Original Fit (Peak PI)')\n",
    "        calculate_and_print_results(\"Original Peak PI Fit Results\", y_peak, x_peak, yerr_peak, popt_peak_orig)\n",
    "    except (RuntimeError, ValueError) as e:\n",
    "        print(f\"\\n--- Original Peak PI Fit FAILED: {e} ---\")\n",
    "\n",
    "    # 2. Custom Fit (with frozen parameters)\n",
    "    try:\n",
    "        # Identify which parameters are free to vary\n",
    "        free_indices = [i for i in range(len(p0_peak_custom)) if i not in params_to_freeze_peak]\n",
    "        \n",
    "        # p0 and bounds only for the free parameters\n",
    "        p0_free = [p0_peak_custom[i] for i in free_indices]\n",
    "        bounds_free = ([bounds_peak_custom[0][i] for i in free_indices], \n",
    "                       [bounds_peak_custom[1][i] for i in free_indices])\n",
    "        \n",
    "        # Wrapper function that fixes the frozen parameters to their p0 values\n",
    "        def wrapped_model(x, *p_free):\n",
    "            p_full = list(p0_peak_custom)\n",
    "            for i, idx in enumerate(free_indices):\n",
    "                p_full[idx] = p_free[i]\n",
    "            return peak_with_erf_tail(x, *p_full)\n",
    "        \n",
    "        popt_free, _ = curve_fit(wrapped_model, x_peak, y_peak, p0=p0_free, sigma=yerr_peak, maxfev=10000, bounds=bounds_free)\n",
    "        \n",
    "        # Reconstruct the full parameter list for plotting and printing\n",
    "        popt_peak_custom = list(p0_peak_custom)\n",
    "        for i, idx in enumerate(free_indices):\n",
    "            popt_peak_custom[idx] = popt_free[i]\n",
    "            \n",
    "        ax.plot(e_fit_plot, peak_with_erf_tail(e_fit_plot, *popt_peak_custom), color='blue', ls=':', lw=2.5, label='Custom Fit (Peak PI)')\n",
    "        calculate_and_print_results(\"Custom Peak PI Fit Results\", y_peak, x_peak, yerr_peak, popt_peak_custom, frozen_indices=params_to_freeze_peak)\n",
    "    except (RuntimeError, ValueError) as e:\n",
    "        print(f\"\\n--- Custom Peak PI Fit FAILED: {e} ---\")\n",
    "\n",
    "# (The Low PI fits section would be updated with the same logic)\n",
    "# --- Low PI Fits ---\n",
    "# (Code for Low PI fits would go here, following the same pattern as the Peak PI fits above)\n",
    "\n",
    "# --- Plot the Data and Finalize ---\n",
    "ax.errorbar(energies, mu_peak_slice, yerr=mu_err_peak_slice, c='deeppink', fmt='.', capsize=3, label=\"Peak PI Data\")\n",
    "#ax.errorbar(energies, mu_low_slice, yerr=mu_err_low_slice, c='cornflowerblue', fmt='s', markersize=4, capsize=3, label=\"Low PI Data\")\n",
    "\n",
    "ax.set(\n",
    "    title=f\"Fit Comparison for Alpha Bin #{alpha_bin_index} ({alpha_low:.2f} ≤ α < {alpha_high:.2f})\",\n",
    "    xlabel=\"Energy (keV)\",\n",
    "    ylabel=r\"$\\mu$\"\n",
    ")\n",
    "ax.legend(fontsize='small')\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abf51de-f4bb-46b4-ad86-78c7960386e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9ee25b-07c7-4bca-99f9-a142d1e285ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
