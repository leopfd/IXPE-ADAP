{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f537e8-8027-4d0a-acba-1983ebe304c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, glob, re\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt # Plotting interface\n",
    "from astropy.io import fits\n",
    "from scipy.optimize import minimize\n",
    "from matplotlib.cm import get_cmap\n",
    "from scipy.signal import find_peaks\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from collections import defaultdict\n",
    "import cmasher as cmr\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import erf\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 400\n",
    "mpl.rcParams['figure.max_open_warning'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f54aad9-9202-436a-8105-2d4eee971ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Globbing Functions ---------- #\n",
    "# Directory containing DU subfolders (du1, du2, du3, ...)\n",
    "ROOT_DIR = \"/Users/leodrake/Library/CloudStorage/Box-Box/IXPE_rmfs/sim_data_mit\"  # Adjusted to your path\n",
    "\n",
    "def list_sim_fits(du_specifier='all', pattern='sim_*_pol_recon*.fits'):\n",
    "    \"\"\"\n",
    "    Return a DU label and sorted FITS paths.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    du_specifier : str or list, optional\n",
    "        If 'all', searches all 'du*' subfolders.\n",
    "        If a string like 'du1', searches only that subfolder.\n",
    "        If a list like ['du1', 'du2'], searches those subfolders.\n",
    "        Defaults to 'all'.\n",
    "    pattern : str, optional\n",
    "        The glob pattern for FITS files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    du_label : str\n",
    "        A label representing the processed DUs (e.g., 'du1', 'du_all', 'du_du1_du2').\n",
    "    files : list\n",
    "        A sorted list of found FITS file paths.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(ROOT_DIR):\n",
    "        raise FileNotFoundError(f\"ROOT_DIR '{ROOT_DIR}' not found.\")\n",
    "\n",
    "    all_dus = [d for d in os.listdir(ROOT_DIR) if d.lower().startswith('du') and os.path.isdir(os.path.join(ROOT_DIR, d))]\n",
    "\n",
    "    if isinstance(du_specifier, str) and du_specifier.lower() == 'all':\n",
    "        subdirs_to_search = all_dus\n",
    "        du_label = 'du123'\n",
    "    elif isinstance(du_specifier, str) and du_specifier in all_dus:\n",
    "        subdirs_to_search = [du_specifier]\n",
    "        du_label = du_specifier\n",
    "    elif isinstance(du_specifier, (list, tuple)):\n",
    "        subdirs_to_search = [d for d in du_specifier if d in all_dus]\n",
    "        if not subdirs_to_search:\n",
    "             raise ValueError(f\"None of the specified DUs {du_specifier} found in {ROOT_DIR}\")\n",
    "        du_label = f'du-{\"-\".join(sorted(subdirs_to_search))}'\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid 'du_specifier': {du_specifier}. Use 'all', a valid DU name, or a list.\")\n",
    "\n",
    "    files = []\n",
    "    for sd in subdirs_to_search:\n",
    "        path = os.path.join(ROOT_DIR, sd, pattern)\n",
    "        files.extend(glob.glob(path))\n",
    "\n",
    "    if not files:\n",
    "        print(f\"Warning: No FITS files found for '{du_label}' with pattern '{pattern}' in {ROOT_DIR}.\")\n",
    "\n",
    "    return du_label, sorted(files)\n",
    "\n",
    "def parse_energy(fname):\n",
    "    \"\"\"Extract energy in keV from 'sim_01000_...' filename.\"\"\"\n",
    "    # Search for energy value (5 digits) in filename\n",
    "    m = re.search(r'sim_(\\d{5})', os.path.basename(fname))\n",
    "    # Convert matched digits to float in keV, or return NaN if no match\n",
    "    return int(m.group(1)) / 1000.0 if m else np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833378d-aff9-4612-aa0c-d8141469943c",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------- Helper Functions ---------- #\n",
    "\n",
    "def bg_flag(alpha, nrg):\n",
    "    \"\"\"Determine if events are background based on alpha and energy.\"\"\"\n",
    "    # Define parameters for background region boundaries\n",
    "    a1, a2, nrg0 = 0.35, 0.7, 2.0  # Thresholds and reference energy\n",
    "    b1 = (1 - a1) / (5.5 - nrg0)  # Slope for first boundary line\n",
    "    b2 = (0.95 - a2) / (8.0 - nrg0)  # Slope for second boundary line\n",
    "    # Identify events above the defined boundary lines as bad\n",
    "    bad = (alpha > (a1 + b1 * (nrg - nrg0))) | (alpha > (a2 + b2 * (nrg - nrg0)))\n",
    "    return ~bad  # Return boolean array: True for good events, False for bad\n",
    "\n",
    "# Extract arrays from simulation FITS (full PI range, no XY cut)\n",
    "def extract_common_data(d, pi_min, pi_max, bgflag=False):\n",
    "    \"\"\"\n",
    "    Extract pi, nrg_pi, alpha, and phi from simulation FITS using the raw PHA channel.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : FITS_rec\n",
    "        The table data for one simulation file.\n",
    "    pi_min, pi_max : float\n",
    "        Full-range PI limits in channel units.\n",
    "    bgflag : bool\n",
    "        If True, apply the background cut via bg_flag().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pi, nrg_pi, alpha, phi : tuple of 1D numpy arrays\n",
    "    \"\"\"\n",
    "    # 1) Raw PHA counts (0–32768)\n",
    "    pha = d['PHA'].astype(float)  # Extract PHA (Pulse Height Amplitude) column\n",
    "    pha2pi = 1.0 / (3000 * 0.04)  # 3000 ADC counts keV^-1, 40eV bins\n",
    "\n",
    "    # 2) Compute PI channels (0–250) and energy keV (0–10)\n",
    "    pi_raw  = pha * pha2pi  # Convert PHA to PI channels\n",
    "    nrg_raw = pha / 3000  # 3000 ADC counts keV^-1: https://doi.org/10.1016/j.astropartphys.2021.102628\n",
    "\n",
    "    # 3) Mask on the full PI range\n",
    "    ok = (pi_raw > pi_min) & (pi_raw <= pi_max)  # Create boolean mask for events within PI range\n",
    "\n",
    "    # 4) Slice out the selected events\n",
    "    pi     = pi_raw[ok]  # PI channels for selected events\n",
    "    nrg_pi = nrg_raw[ok]  # Energy (from PI) for selected events\n",
    "    tl     = d['TRK_M2L'][ok]  # Track length proxy for selected events\n",
    "    tw     = d['TRK_M2T'][ok]  # Track width proxy for selected events\n",
    "    alpha  = (tl - tw) / (tl + tw)  # Calculate alpha parameter (shape parameter)\n",
    "    phi    = d['DETPHI2'][ok]  # Detector phi angle for selected events\n",
    "\n",
    "    # 5) Optional background filtering\n",
    "    if bgflag:\n",
    "        good = bg_flag(alpha, nrg_pi)  # Apply background flag\n",
    "        # Filter data arrays based on good events\n",
    "        pi, nrg_pi, alpha, phi = (\n",
    "            pi[good], nrg_pi[good], alpha[good], phi[good]\n",
    "        )\n",
    "\n",
    "    return pi, nrg_pi, alpha, phi  # Return extracted and filtered arrays\n",
    "\n",
    "def summarize_alpha_vs_pi_bins(pi, alpha, pi_min, pi_max, n_bins=4):\n",
    "    \"\"\"Compute mean alpha in PI bins for a low-PI range.\"\"\"\n",
    "    if n_bins <= 0:\n",
    "        return np.array([]), np.array([]), np.array([], dtype=int)\n",
    "    edges = np.linspace(pi_min, pi_max, n_bins + 1)\n",
    "    centers = (edges[:-1] + edges[1:]) / 2\n",
    "    means = np.full(n_bins, np.nan)\n",
    "    counts = np.zeros(n_bins, dtype=int)\n",
    "    for i in range(n_bins):\n",
    "        sel = (pi >= edges[i]) & (pi < edges[i + 1])\n",
    "        counts[i] = int(np.sum(sel))\n",
    "        if counts[i] > 0:\n",
    "            means[i] = np.nanmean(alpha[sel])\n",
    "    return centers, means, counts\n",
    "\n",
    "def find_pi_peak_band(pi, bin_width=1, min_prominence=0.1, tail_frac=0.3):\n",
    "    \"\"\"\n",
    "    Identify the *highest* peak in the PI histogram and return an expanded band.\n",
    "\n",
    "    The peak band is defined as the full Gaussian core:\n",
    "    - Low edge: where the data begin to exceed the Gaussian fit (tail transition).\n",
    "    - High edge: where the histogram flattens to the background (or the PI limit).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pi              : 1D array of PI channels\n",
    "    bin_width       : width of each PI bin\n",
    "    min_prominence : minimal prominence for find_peaks (fraction of max count)\n",
    "    tail_frac       : fractional excess above the Gaussian to flag tail transition\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pi_min, pi_max : floats\n",
    "        Lower and upper PI channel bounds of the Gaussian core.\n",
    "    \"\"\"\n",
    "    # 1) Histogram the PI data\n",
    "    pi = np.asarray(pi)  # Ensure PI is a numpy array\n",
    "    if pi.size == 0:\n",
    "        return np.nan, np.nan\n",
    "    lo, hi = int(np.nanmin(pi)), int(np.nanmax(pi))  # Min/max PI values for binning\n",
    "    if hi <= lo:\n",
    "        return float(lo), float(hi)\n",
    "    bins = np.arange(lo, hi + bin_width, bin_width)  # Define histogram bins\n",
    "    counts, edges = np.histogram(pi, bins=bins)  # Calculate histogram\n",
    "    centers = (edges[:-1] + edges[1:]) / 2  # Bin centers\n",
    "\n",
    "    peak_idxs, _ = find_peaks(counts, prominence=min_prominence * counts.max())\n",
    "    if peak_idxs.size == 0:\n",
    "        # Fallback: if no peaks found, use the highest single bin\n",
    "        idx = counts.argmax()\n",
    "    else:\n",
    "        # Choose the index of the peak with the largest height among found peaks\n",
    "        idx = peak_idxs[np.argmax(counts[peak_idxs])]\n",
    "\n",
    "    bg_bins = max(5, int(0.1 * len(counts)))\n",
    "    bg_counts = counts[-bg_bins:]\n",
    "    bg_level = float(np.median(bg_counts))\n",
    "    bg_sigma = np.sqrt(bg_level) if bg_level > 0 else 1.0\n",
    "\n",
    "    high_idx = len(counts) - 1\n",
    "    for j in range(idx, len(counts)):\n",
    "        if counts[j] <= bg_level + 2.0 * bg_sigma:\n",
    "            high_idx = j\n",
    "            break\n",
    "\n",
    "    def _gaussian(x, a, mu, sigma):\n",
    "        return a * np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "    x_fit = centers[idx:high_idx + 1]\n",
    "    y_fit = counts[idx:high_idx + 1] - bg_level\n",
    "    fit_mask = y_fit > 0\n",
    "    x_fit = x_fit[fit_mask]\n",
    "    y_fit = y_fit[fit_mask]\n",
    "\n",
    "    mu0 = centers[idx]\n",
    "    sigma0 = max(bin_width, (centers[high_idx] - centers[idx]) / 2.355) if high_idx > idx else bin_width\n",
    "    a0 = max(1.0, counts[idx] - bg_level)\n",
    "\n",
    "    mu = mu0\n",
    "    sigma = sigma0\n",
    "    amp = a0\n",
    "    if x_fit.size >= 3:\n",
    "        try:\n",
    "            popt, _ = curve_fit(_gaussian, x_fit, y_fit, p0=[a0, mu0, sigma0], maxfev=2000)\n",
    "            amp, mu, sigma = popt\n",
    "            sigma = abs(sigma) if sigma != 0 else sigma0\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    gaussian_total = _gaussian(centers, amp, mu, sigma) + bg_level\n",
    "\n",
    "    low_edge = edges[0]\n",
    "    tail_found = False\n",
    "    for j in range(idx, -1, -1):\n",
    "        if counts[j] > gaussian_total[j] * (1.0 + tail_frac):\n",
    "            low_edge = centers[j] + bin_width / 2\n",
    "            tail_found = True\n",
    "            break\n",
    "\n",
    "    if not tail_found:\n",
    "        for j in range(idx, -1, -1):\n",
    "            if counts[j] <= bg_level + 2.0 * bg_sigma:\n",
    "                low_edge = centers[j] + bin_width / 2\n",
    "                break\n",
    "\n",
    "    high_edge = centers[high_idx] + bin_width / 2\n",
    "\n",
    "    if high_edge <= low_edge:\n",
    "        return float(edges[0]), float(edges[-1])\n",
    "    return float(low_edge), float(high_edge)\n",
    "\n",
    "def get_combined_data(file_list):\n",
    "    \"\"\"Reads FITS data from multiple files and concatenates them.\"\"\"\n",
    "    all_data = []\n",
    "    print(f\"    Combining data from {len(file_list)} file(s)...\")\n",
    "    for fname in file_list:\n",
    "        try:\n",
    "            all_data.append(fits.getdata(fname, 1))\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Could not read {os.path.basename(fname)}: {e}\")\n",
    "    if not all_data:\n",
    "        return None\n",
    "    # Ensure all data tables have the same columns before concatenating\n",
    "    if len(all_data) > 1:\n",
    "        first_dtype = all_data[0].dtype\n",
    "        if not all(d.dtype == first_dtype for d in all_data[1:]):\n",
    "            print(\"    Warning: FITS files have different structures. Cannot combine.\")\n",
    "            # For now, we'll return None to indicate failure.\n",
    "            return None\n",
    "    return np.concatenate(all_data)\n",
    "\n",
    "# Build step-plot X,Y coordinates for histograms\n",
    "def step_plot(x, y, binwidth):\n",
    "    \"\"\"Create x, y coordinates for a step plot from histogram data.\"\"\"\n",
    "    xsteps, ysteps = [], []  # Initialize lists for step plot coordinates\n",
    "    # For each bin, create two x (left/right edge) and two y (height) points\n",
    "    for xi, yi in zip(x, y):\n",
    "        xsteps += [xi - binwidth/2, xi + binwidth/2]\n",
    "        ysteps += [yi, yi]\n",
    "    return xsteps, ysteps\n",
    "\n",
    "# Plot XY vs Time and φ vs α (Unchanged, but not explicitly used by generate_summary_plots)\n",
    "def plot_xy_vs_t(d, pi_min, pi_max, alpha_min, title):\n",
    "    \"\"\"Generates plots for XY vs Time and phi vs alpha.\"\"\"\n",
    "    # Extract data; bgflag is False by default here\n",
    "    pi, nrg_pi, alpha, phi = extract_common_data(d, pi_min, pi_max, bgflag=False)\n",
    "    time0 = d['TIME'] - np.min(d['TIME'])  # Normalize time to start from 0\n",
    "    X, Y = d['ABSX'], d['ABSY']  # Absolute X and Y coordinates\n",
    "    figs = []  # List to store generated figures\n",
    "\n",
    "    # φ histogram\n",
    "    dphi = 0.001 * np.pi  # Bin width for phi histogram\n",
    "    phist, edges = np.histogram(phi, bins=np.arange(-np.pi, np.pi + dphi, dphi))  # Compute phi histogram\n",
    "    pval = (edges[:-1] + edges[1:]) / 2  # Phi bin centers\n",
    "    xs, ys = step_plot(pval, phist, dphi)  # Get step plot coordinates\n",
    "    fig, ax = plt.subplots(); ax.plot(xs, ys); ax.set(title=f'{title} – φ dist', xlabel='φ'); figs.append(fig)\n",
    "\n",
    "    # φ vs α scatter plot\n",
    "    fig, ax = plt.subplots(); ax.scatter(phi, alpha, s=1)  # s=1 for small marker size\n",
    "    ax.set(title=f'{title} – φ vs α', xlabel='φ', ylabel='α'); figs.append(fig)\n",
    "    return figs\n",
    "\n",
    "# Plot PI & α distributions (Unchanged)\n",
    "def plot_pi_alpha(d, pi_min, pi_max, title, dist_component='Full', a_color='blueviolet'):\n",
    "    \"\"\"Generates plots for PI and alpha distributions.\"\"\"\n",
    "    # Extract data; bgflag is False by default here\n",
    "    pi, _, alpha, _ = extract_common_data(d, pi_min, pi_max, bgflag=False)\n",
    "    pi_full, _, alpha_full, _ = extract_common_data(d, 1, 374, bgflag=False)\n",
    "    figs = []  # List to store generated figures\n",
    "\n",
    "    # PI distribution plot\n",
    "    phist, edges = np.histogram(pi, bins=np.arange(pi_min, pi_max+1, 1))  # PI histogram (bin width 1)\n",
    "    pval = (edges[:-1] + edges[1:]) / 2  # PI bin centers\n",
    "    xs, ys = step_plot(pval, phist, 1)  # Get step plot coordinates\n",
    "    fig, ax = plt.subplots(); ax.plot(xs, ys, 'k', label=f'{dist_component} distribution')\n",
    "    ax.set(title=f'{title} – PI dist', xlabel='PI'); figs.append(fig)\n",
    "    ax.legend()\n",
    "\n",
    "    # α distribution plot\n",
    "    ahist, edges = np.histogram(alpha, bins=np.arange(0, 1.0, 0.01))  # Alpha histogram (bin width 0.01)\n",
    "    aval = (edges[:-1] + edges[1:]) / 2  # Alpha bin centers\n",
    "    xs, ys = step_plot(aval, ahist, 0.01)  # Get step plot coordinates\n",
    "    full_alpha_max = None\n",
    "    if alpha_full.size:\n",
    "        full_ahist, _ = np.histogram(alpha_full, bins=np.arange(0, 1.0, 0.01))\n",
    "        full_alpha_max = full_ahist.max()\n",
    "    fig, ax = plt.subplots(); ax.plot(xs, ys, a_color, label=f'{dist_component} distribution')\n",
    "    ax.set(title=f'{title} – α dist', xlabel='α'); figs.append(fig)\n",
    "    if full_alpha_max is not None and full_alpha_max > 0:\n",
    "        ax.set_ylim(0, full_alpha_max * 1.05)\n",
    "    legend = ax.legend()\n",
    "    if pi_full.size:\n",
    "        fig.canvas.draw()\n",
    "        renderer = fig.canvas.get_renderer()\n",
    "        legend_bbox = legend.get_window_extent(renderer=renderer).transformed(ax.transAxes.inverted())\n",
    "        width, height = 0.35, 0.35\n",
    "        pad = 0.02\n",
    "        gap = 0.02\n",
    "        x0 = min(max(legend_bbox.x0, pad), 1.0 - width - pad)\n",
    "        y0 = legend_bbox.y0 - gap - height\n",
    "        if y0 < pad:\n",
    "            y0 = min(legend_bbox.y1 + gap, 1.0 - height - pad)\n",
    "        y0 = min(max(y0, pad), 1.0 - height - pad)\n",
    "        axins = inset_axes(\n",
    "            ax,\n",
    "            width=f\"{int(width*100)}%\",\n",
    "            height=f\"{int(height*100)}%\",\n",
    "            bbox_to_anchor=(x0, y0, 1, 1),\n",
    "            bbox_transform=ax.transAxes,\n",
    "            loc=\"lower left\",\n",
    "            borderpad=0,\n",
    "        )\n",
    "        full_pi_min, full_pi_max = 1, 374\n",
    "        phist_full, edges_full = np.histogram(pi_full, bins=np.arange(full_pi_min, full_pi_max + 1, 1))\n",
    "        pval_full = (edges_full[:-1] + edges_full[1:]) / 2\n",
    "        xs_full, ys_full = step_plot(pval_full, phist_full, 1)\n",
    "        axins.plot(xs_full, ys_full, \"k\", lw=0.8)\n",
    "        axins.axvline(pi_min, color=a_color, linestyle=\"--\", lw=1)\n",
    "        axins.axvline(pi_max, color=a_color, linestyle=\"--\", lw=1)\n",
    "        axins.set_xlim(full_pi_min, full_pi_max)\n",
    "        axins.set_xticks([]); axins.set_yticks([])\n",
    "        axins.patch.set_alpha(0.7)\n",
    "        if dist_component.lower().startswith('tail') and pi_max > full_pi_min:\n",
    "            axins.axvspan(full_pi_min, pi_max, color=a_color, alpha=0.05)\n",
    "        elif dist_component.lower().startswith('peak'):\n",
    "            axins.axvspan(pi_min, pi_max, color=a_color, alpha=0.05)\n",
    "    return figs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c4adb-711c-4e88-8c86-c8f1a3754a5e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ---------- Fitting Functions ---------- #\n",
    "\n",
    "def minimizer(func, p0, args=(), tol=1e-6):\n",
    "    \"\"\"Wrapper for scipy.optimize.minimize using Nelder-Mead method.\"\"\"\n",
    "    # Perform minimization\n",
    "    res = minimize(func, p0, args=args, method='Nelder-Mead', tol=tol)\n",
    "    return res.x  # Return the optimal parameters\n",
    "\n",
    "def safe_polar_likelihood_1d(param, ci):\n",
    "    \"\"\"Calculates -2*log-likelihood for 1D polarization, with penalty for invalid parameters.\"\"\"\n",
    "    # param is modulation factor mu, ci is cos(2*phase_i)\n",
    "    val = 1 + param * ci  # Argument of the logarithm in likelihood function\n",
    "    # If any argument is non-positive, return a large penalty value (high cost)\n",
    "    if np.any(val <= 0):\n",
    "        return 1e6 + 1e3 * abs(param)\n",
    "    return -2 * np.sum(np.log(val))  # Standard -2*log-likelihood\n",
    "\n",
    "def invert_matrix(matrix):\n",
    "    \"\"\"Safely inverts a matrix, handling potential LinAlgError.\"\"\"\n",
    "    try:\n",
    "        return 0, np.linalg.inv(matrix)  # Return 0 (success) and inverted matrix\n",
    "    except np.linalg.LinAlgError:\n",
    "        return 1, None  # Return 1 (error) and None\n",
    "\n",
    "def polar_likelihood(param, evtq, evtu):\n",
    "    \"\"\"Calculates -2*log-likelihood for Stokes Q, U parameters.\"\"\"\n",
    "    q, u = param  # Unpack Stokes Q and U from parameters\n",
    "    arg = 1 + q*evtq + u*evtu  # Argument of log: 1 + Q*cos(2*phi) + U*sin(2*phi)\n",
    "    # Raise error if any argument is non-positive (log undefined)\n",
    "    if np.any(arg <= 0):\n",
    "        raise ValueError(\"Non-positive argument encountered in log\")\n",
    "    return -2 * np.sum(np.log(arg))  # Sum of -2*log values for all events\n",
    "\n",
    "def polar_evpa_likelihood(param, evtq, evtu):\n",
    "    \"\"\"Calculates -2*log-likelihood for polarization degree (P) and angle (EVPA).\"\"\"\n",
    "    dtor = np.pi/180.0  # Degrees to radians conversion factor\n",
    "    # Convert P, EVPA to Q, U\n",
    "    q = param[0]*np.cos(2*param[1]*dtor)  # param[0] is P, param[1] is EVPA\n",
    "    u = param[0]*np.sin(2*param[1]*dtor)\n",
    "    arg = 1 + q*evtq + u*evtu  # Argument of log\n",
    "    # Raise error if any argument is non-positive\n",
    "    if np.any(arg <= 0):\n",
    "        raise ValueError(\"Non-positive argument encountered in log\")\n",
    "    return -2 * np.sum(np.log(arg))  # Sum of -2*log values\n",
    "\n",
    "def pderiv(func, x, i, dx):\n",
    "    \"\"\"Computes partial derivative of func w.r.t. x[i] using central difference.\"\"\"\n",
    "    x0, x1 = x.copy(), x.copy()  # Create copies of parameter vector\n",
    "    x0[i] -= 0.5*dx  # Perturb parameter x[i] backward\n",
    "    x1[i] += 0.5*dx  # Perturb parameter x[i] forward\n",
    "    return (func(x1) - func(x0)) / dx  # Central difference formula\n",
    "\n",
    "def pderiv2(func, x, dx):\n",
    "    \"\"\"Computes the Hessian matrix (matrix of second partial derivatives) of func.\"\"\"\n",
    "    n = len(x)  # Number of parameters\n",
    "    H = np.zeros((n,n))  # Initialize Hessian matrix\n",
    "    # Iterate over upper triangle of the Hessian\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            x0, x1 = x.copy(), x.copy()\n",
    "            # Perturb x[i] to compute derivative w.r.t x[j]\n",
    "            x0[i] -= 0.5*dx[i]; x1[i] += 0.5*dx[i]\n",
    "            # Compute partial derivative of (d func / d x[j]) at x0 and x1\n",
    "            pd0 = pderiv(func, x0, j, dx[j])\n",
    "            pd1 = pderiv(func, x1, j, dx[j])\n",
    "            # Second derivative (d^2 func / dx[i]dx[j])\n",
    "            H[i,j] = (pd1 - pd0)/dx[i]\n",
    "    # Symmetrize the Hessian (H[j,i] = H[i,j])\n",
    "    return H + H.T - np.diag(np.diagonal(H))\n",
    "\n",
    "def likelihood(evtq, evtu):\n",
    "    \"\"\"Estimates Stokes Q, U, polarization degree, EVPA, and errors.\"\"\"\n",
    "    # evtq = cos(2*phi_i), evtu = sin(2*phi_i) for each event i\n",
    "    sumq2 = np.sum(evtq**2)  # Sum of cos^2(2*phi_i)\n",
    "    sumu2 = np.sum(evtu**2)  # Sum of sin^2(2*phi_i)\n",
    "\n",
    "    # Simple (approximate) initial estimates for Q, U errors\n",
    "    qu_err_init = np.array([1/np.sqrt(sumq2) if sumq2>0 else np.nan,\n",
    "                            1/np.sqrt(sumu2) if sumu2>0 else np.nan])\n",
    "    # Simple initial estimates for Q, U values\n",
    "    qu0 = np.array([np.sum(evtq)*(qu_err_init[0]**2) if sumq2>0 else 0.0,\n",
    "                    np.sum(evtu)*(qu_err_init[1]**2) if sumu2>0 else 0.0])\n",
    "\n",
    "    # Fit Q,U using Nelder-Mead minimization of polar_likelihood\n",
    "    initial_like_val = 0  # Initialize\n",
    "    try:\n",
    "        initial_like_val = polar_likelihood(qu0, evtq, evtu)  # Calculate initial likelihood\n",
    "        # Set tolerance for minimizer based on initial likelihood value\n",
    "        tol = abs(0.01 / initial_like_val) if initial_like_val != 0 and (sumq2 + sumu2 > 0) else 1e-6\n",
    "    except ValueError:  # Catch potential log error if qu0 is problematic\n",
    "        tol = 1e-6\n",
    "\n",
    "    try:\n",
    "        # Perform minimization to find best-fit Q, U\n",
    "        qu = minimizer(polar_likelihood, qu0, args=(evtq, evtu), tol=tol)\n",
    "    except ValueError:  # If minimizer fails due to initial values\n",
    "        qu = qu0.copy()  # Use initial estimates\n",
    "    except Exception:  # Catch other minimization errors\n",
    "        qu = qu0.copy()  # Use initial estimates\n",
    "\n",
    "    # Error estimation from Hessian matrix of the likelihood function\n",
    "    qu_err_fit = qu_err_init.copy()  # Default to initial error estimates\n",
    "    try:\n",
    "        # Step sizes for numerical differentiation, use initial errors or small default\n",
    "        hess_dx = qu_err_init if np.all(np.isfinite(qu_err_init)) and np.all(qu_err_init > 0) else np.ones_like(qu_err_init)*1e-3\n",
    "        # Compute Hessian (0.5 factor for -2logL)\n",
    "        H = 0.5 * pderiv2(lambda x: polar_likelihood(x, evtq, evtu), qu, hess_dx)\n",
    "        err_code, M = invert_matrix(H)  # Invert Hessian to get covariance matrix\n",
    "        if M is not None:  # If inversion successful\n",
    "            qu_err_fit = np.sqrt(np.abs(np.diag(M)))  # Errors are sqrt of diagonal elements\n",
    "        else:  # If inversion failed\n",
    "            qu_err_fit = np.array([np.nan, np.nan])\n",
    "    except:  # Catch any other errors during Hessian calculation/inversion\n",
    "        qu_err_fit = np.array([np.nan, np.nan])\n",
    "\n",
    "    # Calculate polarization degree (poln) and electric vector position angle (evpa)\n",
    "    poln = np.linalg.norm(qu)  # Polarization degree: sqrt(Q^2 + U^2)\n",
    "    evpa = 0.5 * np.arctan2(qu[1], qu[0]) * (180.0/np.pi)  # EVPA in degrees\n",
    "\n",
    "    # Minimum Detectable Polarization (MDP) estimate\n",
    "    mdp = 4.29/np.sqrt(sumq2+sumu2) if sumq2+sumu2>0 else np.nan\n",
    "    # Note: Errors for poln and evpa are not calculated here via propagation from Q,U errors.\n",
    "    # Return initial Q,U (qu0), their errors (qe0=qu_err_init),\n",
    "    return qu0, qu_err_init, qu, qu_err_fit, poln, np.nan, evpa, np.nan, np.nan, mdp\n",
    "\n",
    "def fit_mu_alpha(d, pi_min, pi_max, nalpha, title, bgflag=False):\n",
    "    \"\"\"\n",
    "    IDL-style fit_mu_alpha for SIM data, full-PI.\n",
    "    Fits modulation factor mu in bins of alpha.\n",
    "\n",
    "    Returns: (outputs_dict, list_of_figures)\n",
    "    \"\"\"\n",
    "    # 1) Extract common data arrays (pi, energy, alpha, phi)\n",
    "    pi, nrg_pi, alpha, phi = extract_common_data(d, pi_min, pi_max, bgflag)\n",
    "    figs = []  # Initialize list to store figures\n",
    "\n",
    "    # 3) Phi distribution + model (Step 2 from IDL might be implicit or handled elsewhere)\n",
    "    dphi = 0.001 * np.pi  # Bin width for phi histogram\n",
    "    phist, edges = np.histogram(phi, bins=np.arange(-np.pi, np.pi + dphi, dphi))  # Compute phi histogram\n",
    "    phival = (edges[:-1] + edges[1:]) / 2  # Phi bin centers\n",
    "    evtq = np.cos(2*phi); evtu = np.sin(2*phi)  # Per-event Q and U proxies\n",
    "\n",
    "    # Handle cases with no events in the selected PI band\n",
    "    if len(evtq) == 0 or len(evtu) == 0:\n",
    "        # Prepare an empty output structure\n",
    "        outputs = {\n",
    "            \"mu_noweight\": np.nan, \"mu_noweight_err\": np.nan,\n",
    "            \"alpha_bins\": (np.arange(nalpha)+0.5)*(1.0/float(nalpha)),\n",
    "            \"mu_bins\": np.full(nalpha, np.nan), \"mu_bins_err\": np.full(nalpha, np.nan),\n",
    "            \"nevt_bins\": np.zeros(nalpha, int)\n",
    "        }\n",
    "        # Create an empty placeholder plot for phi distribution\n",
    "        phi_fig, phi_ax = plt.subplots()\n",
    "        phi_ax.set(title=rf\"{title} – $\\phi$ Dist. (No Data)\", xlabel=r\"$\\phi$\")\n",
    "        figs.append(phi_fig)\n",
    "        return outputs, figs\n",
    "\n",
    "    # Calculate overall Q, U, polarization, etc. using the likelihood function\n",
    "    qu0, qe0, qu, qe, mu_nw, mu_nw_err, evpa, evpa_err, dlike, mdp = likelihood(evtq, evtu)\n",
    "\n",
    "    # Correct normalization: number of φ-bins = len(phist)\n",
    "    nbin_phi_hist = len(phist)  # Number of bins in the phi histogram\n",
    "    model = np.zeros_like(phival)  # Initialize model array\n",
    "    # Calculate model if there are events and histogram bins\n",
    "    if nbin_phi_hist > 0 and len(phi) > 0:\n",
    "        # Model: N_total * (1 + Q*cos(2*phi_val) + U*sin(2*phi_val)) / N_bins\n",
    "        model = len(phi) * (1 + qu[0]*np.cos(2*phival) + qu[1]*np.sin(2*phival)) / nbin_phi_hist\n",
    "    else:  # If no phi values, fill model with NaNs\n",
    "        model.fill(np.nan)\n",
    "\n",
    "    # Plot phi distribution with the fitted model\n",
    "    xs2, ys2 = step_plot(phival, phist, dphi)  # Coordinates for step plot of histogram\n",
    "    phi_fig, phi_ax = plt.subplots()\n",
    "    phi_ax.plot(xs2, ys2, 'darkseagreen', alpha=0.75, label=\"Data\")  # Plot histogram\n",
    "    phi_ax.plot(phival, model, 'k', label=\"Model\")  # Plot model\n",
    "    phi_ax.set(title=rf\"{title} – $\\phi$ Dist.\", xlabel=r\"$\\phi$\")\n",
    "    # phi_ax.legend() # Optional: add legend if desired\n",
    "    figs.append(phi_fig)\n",
    "\n",
    "    # Handle qu being all NaNs (e.g., if likelihood fit failed or no data)\n",
    "    phase = (phi - 0.5*np.arctan2(qu[1], qu[0])) % np.pi if not np.all(np.isnan(qu)) else np.full_like(phi, np.nan)\n",
    "    dalpha = 1.0/float(nalpha)  # Width of each alpha bin\n",
    "    alpha_centers = (np.arange(nalpha)+0.5)*dalpha  # Center of each alpha bin\n",
    "\n",
    "    mu_bins = np.zeros(nalpha)  # Initialize array for mu values in alpha bins\n",
    "    mu_err  = np.full(nalpha, np.nan)  # Initialize array for mu errors (with NaNs)\n",
    "    nevt    = np.zeros(nalpha, int)  # Initialize array for number of events in alpha bins\n",
    "\n",
    "    MIN_EVENTS = 100  # Minimum number of events required in an alpha bin to perform fit\n",
    "\n",
    "    # Iterate over alpha bins to fit mu\n",
    "    for i, ac in enumerate(alpha_centers):\n",
    "        # Select events within the current alpha bin\n",
    "        sel = np.where((alpha >= ac - dalpha/2) & (alpha < ac + dalpha/2))[0]\n",
    "        nevt[i] = sel.size  # Number of events in this bin\n",
    "\n",
    "        # Only fit this bin if enough events and phase is valid\n",
    "        if nevt[i] < MIN_EVENTS or np.all(np.isnan(phase)):\n",
    "            mu_bins[i] = np.nan  # Not enough events or invalid phase, set mu to NaN\n",
    "            continue  # Skip to next alpha bin\n",
    "\n",
    "        ci = np.cos(2*phase[sel])  # cos(2*aligned_phase) for selected events\n",
    "        # Initial guess for mu parameter for 1D likelihood fit\n",
    "        p0 = np.array([np.sum(ci)/np.sum(ci*ci)]) if np.sum(ci*ci)>0 else np.array([0.0])\n",
    "        # Fit mu using safe_polar_likelihood_1d minimizer\n",
    "        mu_bins[i] = minimizer(safe_polar_likelihood_1d, p0, args=(ci,), tol=1e-6)[0]\n",
    "        # Calculate error for mu_bins[i] from the likelihood curvature (1/sqrt(Fisher_information))\n",
    "        denom = np.sum(ci*ci/((1+mu_bins[i]*ci)**2))  # Denominator for error calculation\n",
    "        mu_err[i] = np.sqrt(1/denom) if denom>0 else np.nan\n",
    "\n",
    "        # (Currently, this plot is generated for every valid bin)\n",
    "        pbin = 0.01*np.pi  # Bin width for phase histogram in this alpha bin\n",
    "        ph2, ed2 = np.histogram(phase[sel], bins=np.arange(0,np.pi+pbin,pbin))  # Histogram of aligned phases\n",
    "        pv2 = (ed2[:-1]+ed2[1:])/2  # Phase bin centers\n",
    "        norm = ph2/ph2.sum() if ph2.sum() > 0 else ph2  # Normalized counts (density)\n",
    "        model_ph = 0.01*(1+mu_bins[i]*np.cos(2*pv2))  # Model for phase distribution: (1 + mu*cos(2*phase_val)) * dPhase\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        xs3, ys3 = step_plot(pv2, norm, pbin)  # Coordinates for step plot\n",
    "        ax.plot(xs3, ys3, 'darkseagreen', label=\"Data\")\n",
    "        ax.plot(pv2, model_ph, 'k--', label=rf\"$\\mu={mu_bins[i]:.2f}$\")  # Plot model with fitted mu\n",
    "        ax.set(title=f\"{title} – {ac-dalpha/2:.1f}<α<{ac+dalpha/2:.1f}\",\n",
    "               xlabel=\"Phase (rel. to EVPA)\", ylabel=\"Normalized Counts\")\n",
    "        ax.legend()\n",
    "        figs.append(fig)\n",
    "\n",
    "    # Store results in a dictionary\n",
    "    outputs = {\n",
    "        \"mu_noweight\": mu_nw,  # Overall modulation factor (no alpha weighting)\n",
    "        \"mu_noweight_err\": mu_nw_err,  # Error for mu_noweight\n",
    "        \"alpha_bins\": alpha_centers,  # Centers of alpha bins\n",
    "        \"mu_bins\": mu_bins,  # Fitted mu in each alpha bin\n",
    "        \"mu_bins_err\": mu_err,  # Error of mu in each alpha bin\n",
    "        \"nevt_bins\": nevt  # Number of events in each alpha bin\n",
    "    }\n",
    "    return outputs, figs  # Return results dictionary and list of figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f31fe-9b33-4a59-9b87-2b486a15bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Summary Figures ---------- #\n",
    "import matplotlib.patheffects as pe  # Explicit import for text effects used in stackplot labels\n",
    "\n",
    "def peak_with_erf_tail(E, A_peak, E_break, alpha1, alpha2, C_tail, E_trans, W_trans):\n",
    "    \"\"\"\n",
    "    A broken power-law peak that smoothly transitions to a flat, constant tail.\n",
    "    C_tail: The constant height of the flat tail.\n",
    "    E_trans: The energy where the transition to the tail occurs.\n",
    "    W_trans: The width (speed) of the erf transition.\n",
    "    \"\"\"\n",
    "    # Create the broken power-law core shape\n",
    "    if E_break <= 0:\n",
    "        return np.full_like(E, np.inf)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        term1 = (E / E_break)**(-alpha1)\n",
    "        term2 = (E / E_break)**(-alpha2)\n",
    "        peak = A_peak * (term1 + term2)**(-1)\n",
    "    \n",
    "    # Create the erf-based switch (goes smoothly from 0 to 1)\n",
    "    switch = (1 + erf((E - E_trans) / W_trans)) / 2.0\n",
    "    \n",
    "    # Combine the two parts\n",
    "    return peak * (1 - switch) + C_tail * switch\n",
    "\n",
    "def compute_model(eplot, aa=-0.28, bb=0.2, cc=0.21, dd=1./24.):\n",
    "    \"\"\"\n",
    "    Compute the IDL-like empirical model for modulation factor mu vs. energy.\n",
    "    Model: mu = (1 / [(-aa - bb*E)^-4 + (-cc - dd*E)^-4])^0.25,\n",
    "    handling non-finite values.\n",
    "    \"\"\"\n",
    "    # Calculate the two terms in the denominator\n",
    "    term1 = (-aa - bb * eplot)**(-4)\n",
    "    term2 = (-cc - dd * eplot)**(-4)\n",
    "    # Compute model, suppressing errors for invalid operations (e.g., division by zero)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        model = (1.0 / (term1 + term2))**0.25\n",
    "        model[~np.isfinite(model)] = np.nan  # Set non-finite results (e.g. from negative bases) to NaN\n",
    "    return model\n",
    "\n",
    "def generate_summary_sim_plots(energies_list, peak_results_list, low_results_list, du_label, filename=\"summary_plots_sim.pdf\"):\n",
    "    \"\"\"\n",
    "    Create a multipage PDF of summary plots for simulation data using only the Erf Tail model.\n",
    "    \"\"\"\n",
    "    def result_is_valid(res):\n",
    "        \"\"\"Helper function to check if a result dictionary is valid (contains finite mu_noweight).\"\"\"\n",
    "        return (\n",
    "            res is not None and\n",
    "            (\"mu_noweight\" in res) and\n",
    "            np.isfinite(res[\"mu_noweight\"])\n",
    "        )\n",
    "\n",
    "    valid_indices = [\n",
    "        i for i, (p, l, e) in enumerate(zip(peak_results_list, low_results_list, energies_list))\n",
    "        if result_is_valid(p) and result_is_valid(l) and np.isfinite(e)\n",
    "    ]\n",
    "\n",
    "    if not valid_indices:\n",
    "        print(f\"Error: No valid results found for DU {du_label}. Cannot generate summary plots.\")\n",
    "        return\n",
    "\n",
    "    energies = np.array([energies_list[i] for i in valid_indices])\n",
    "    peak_results = [peak_results_list[i] for i in valid_indices]\n",
    "    low_results  = [low_results_list[i] for i in valid_indices]\n",
    "\n",
    "    if len(energies) == 0:\n",
    "        print(f\"Error: No valid data remain after filtering for DU {du_label}. Cannot generate summary plots.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nGenerating summary plots for {len(energies)} valid simulations ({du_label}) in '{filename}'...\")\n",
    "\n",
    "    tot_hi = np.array([np.sum(res[\"nevt_bins\"]) if \"nevt_bins\" in res and res[\"nevt_bins\"] is not None else 0 for res in peak_results])\n",
    "    tot_lo = np.array([np.sum(res[\"nevt_bins\"]) if \"nevt_bins\" in res and res[\"nevt_bins\"] is not None else 0 for res in low_results])\n",
    "    valid_evt_mask = ((tot_hi + tot_lo) > 0)\n",
    "    mu_noweight_hi = np.array([res[\"mu_noweight\"] for res in peak_results])\n",
    "    mu_noweight_hi_err = np.array([res[\"mu_noweight_err\"] for res in peak_results])\n",
    "    mu_noweight_lo = np.array([res[\"mu_noweight\"] for res in low_results])\n",
    "    mu_noweight_lo_err = np.array([res[\"mu_noweight_err\"] for res in low_results])\n",
    "    mu_noweight_combined = np.full_like(mu_noweight_hi, np.nan)\n",
    "    numerator_combined = np.nan_to_num(tot_hi * mu_noweight_hi) + np.nan_to_num(tot_lo * mu_noweight_lo)\n",
    "    denominator_combined = tot_hi + tot_lo\n",
    "    mu_noweight_combined[valid_evt_mask] = numerator_combined[valid_evt_mask] / denominator_combined[valid_evt_mask]\n",
    "\n",
    "    eplot = np.linspace(min(energies), max(energies), 500)\n",
    "    mu_model_orig   = compute_model(eplot, aa=-0.28, bb=0.2, cc=0.21, dd=1./24.)\n",
    "    mu_model_better = compute_model(eplot, aa=-0.28, bb=0.2, cc=0.21, dd=1./18.5)\n",
    "\n",
    "    with PdfPages(filename) as pdf:\n",
    "        fig1, ax1 = plt.subplots();\n",
    "        ax1.errorbar(energies, mu_noweight_hi, yerr=mu_noweight_hi_err, c='deeppink', fmt='.', capsize=3, label=\"Peak PI\")\n",
    "        ax1.errorbar(energies, mu_noweight_lo, yerr=mu_noweight_lo_err, c='cornflowerblue', fmt='s', markersize=3, capsize=3, label=\"Tail PI\")\n",
    "        ax1.plot(energies, mu_noweight_combined, 'd', markersize=3, c='blueviolet', label=\"All PI\")\n",
    "        ax1.plot(eplot, mu_model_better, 'k', label=\"Peak Only Model\", zorder=10)\n",
    "        ax1.plot(eplot, mu_model_orig, 'k--', label=\"All PI Model (Di Marco+)\", zorder=9)\n",
    "        ax1.set(xlabel=\"Energy (keV)\", ylabel=r\"$\\mu$\", title=f\"IXPE Simulations {du_label}\"); ax1.legend()\n",
    "        pdf.savefig(fig1); plt.close(fig1)\n",
    "\n",
    "        a_ref, b_ref = 0.05, 0.8; a_line_ref = np.linspace(0,1,200); mu_line_ref = a_ref + b_ref*a_line_ref\n",
    "        norm = plt.Normalize(vmin=np.min(energies), vmax=np.max(energies))\n",
    "        fig2, ax2 = plt.subplots();\n",
    "        for E_val, res_dict in zip(energies, peak_results):\n",
    "            mask = res_dict[\"nevt_bins\"] > 100\n",
    "            ax2.plot(res_dict[\"alpha_bins\"][mask], res_dict[\"mu_bins\"][mask], color=plt.cm.rainbow_r(norm(E_val)), lw=1)\n",
    "        ax2.plot(a_line_ref, mu_line_ref, 'k--', linewidth=3, label=\"Di Marco et al.\")\n",
    "        ax2.set(title=\"Peak PI: μ vs α\", xlabel=\"α\", ylabel=\"μ\"); ax2.legend()\n",
    "        fig2.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=\"rainbow_r\"), ax=ax2, label=\"Energy (keV)\")\n",
    "        plt.tight_layout(); pdf.savefig(fig2); plt.close(fig2)\n",
    "\n",
    "        fig3, ax3 = plt.subplots()\n",
    "        for E_val, res_dict in zip(energies, low_results):\n",
    "            mask = res_dict[\"nevt_bins\"] > 100\n",
    "            ax3.plot(res_dict[\"alpha_bins\"][mask], res_dict[\"mu_bins\"][mask], color=plt.cm.rainbow_r(norm(E_val)), lw=1)\n",
    "        ax3.plot(a_line_ref, mu_line_ref, 'k--', linewidth=3, label=\"Di Marco et al.\")\n",
    "        ax3.set(title=\"Tail PI: μ vs α\", xlabel=\"α\", ylabel=\"μ\"); ax3.legend()\n",
    "        fig3.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=\"rainbow_r\"), ax=ax3, label=\"Energy (keV)\")\n",
    "        plt.tight_layout(); pdf.savefig(fig3); plt.close(fig3)\n",
    "        \n",
    "        fig4, ax4 = plt.subplots()\n",
    "        a_model_linear, b_model_linear = 0.05, 0.8\n",
    "        for E_val, res_dict in zip(energies, peak_results):\n",
    "            mask = res_dict[\"nevt_bins\"] > 100\n",
    "            mu_improvement = res_dict[\"mu_bins\"] - (a_model_linear + b_model_linear * res_dict[\"alpha_bins\"])\n",
    "            ax4.plot(res_dict[\"alpha_bins\"][mask], mu_improvement[mask], color=plt.cm.rainbow_r(norm(E_val)), linewidth=1)\n",
    "        ax4.axhline(0, color='k', linestyle='--', linewidth=3)\n",
    "        ax4.set(title=\"Peak PI: Model Improvement\", xlabel=\"α\", ylabel=\"μ − model (linear)\")\n",
    "        fig4.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=\"rainbow_r\"), ax=ax4, label=\"Energy (keV)\")\n",
    "        plt.tight_layout(); pdf.savefig(fig4); plt.close(fig4)\n",
    "\n",
    "        fig4b, ax4b = plt.subplots()\n",
    "        series_plotted = 0\n",
    "        for E_val, res_dict in zip(energies, low_results):\n",
    "            centers = res_dict.get(\"alpha_pi_low_centers\")\n",
    "            means = res_dict.get(\"alpha_pi_low_mean\")\n",
    "            if centers is None or means is None:\n",
    "                continue\n",
    "            centers = np.asarray(centers)\n",
    "            means = np.asarray(means)\n",
    "            counts = res_dict.get(\"alpha_pi_low_counts\")\n",
    "            if counts is None:\n",
    "                mask = np.isfinite(means)\n",
    "            else:\n",
    "                mask = (np.asarray(counts) > 0) & np.isfinite(means)\n",
    "            if np.any(mask):\n",
    "                ax4b.plot(centers[mask], means[mask], color=plt.cm.rainbow_r(norm(E_val)), lw=1)\n",
    "                series_plotted += 1\n",
    "        if series_plotted == 0:\n",
    "            print(\"   -> Tail PI alpha vs PI bin: no valid bins to plot.\")\n",
    "        else:\n",
    "            ax4b.set(title=\"Tail PI: mean alpha vs PI bin\", xlabel=\"PI bin center\", ylabel=\"Mean alpha\")\n",
    "            fig4b.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=\"rainbow_r\"), ax=ax4b, label=\"Energy (keV)\")\n",
    "            plt.tight_layout(); pdf.savefig(fig4b)\n",
    "        plt.close(fig4b)\n",
    "\n",
    "        \n",
    "        # Helper variables\n",
    "        n_alpha_bins = len(peak_results[0][\"alpha_bins\"])\n",
    "        alpha_bin_centers = peak_results[0][\"alpha_bins\"]\n",
    "        d_alpha = 1.0 / n_alpha_bins\n",
    "        \n",
    "        labels = [fr\"${center-d_alpha/2.0:.2f} \\leq \\alpha < {center+d_alpha/2.0:.2f}$\" for center in alpha_bin_centers]\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, n_alpha_bins))\n",
    "\n",
    "        # We will normalize so the total stack height is 1.0\n",
    "        stack_data = [[] for _ in range(2 * n_alpha_bins)]\n",
    "        energies_for_stackplot = []\n",
    "        boundary_line = []  # To draw the line between Low and Peak\n",
    "        \n",
    "        for E_val, res_p, res_l in zip(energies, peak_results, low_results):\n",
    "            n_peak = res_p[\"nevt_bins\"]\n",
    "            n_low  = res_l[\"nevt_bins\"]\n",
    "            \n",
    "            tot_peak = np.sum(n_peak)\n",
    "            tot_low  = np.sum(n_low)\n",
    "            total = tot_peak + tot_low\n",
    "            \n",
    "            if total > 0:\n",
    "                energies_for_stackplot.append(E_val)\n",
    "                \n",
    "                # Fractions of the TOTAL for Low PI (Bottom half)\n",
    "                fracs_low = n_low / total\n",
    "                for i in range(n_alpha_bins):\n",
    "                    stack_data[i].append(fracs_low[i])\n",
    "                    \n",
    "                # Fractions of the TOTAL for Peak PI (Top half)\n",
    "                fracs_peak = n_peak / total\n",
    "                for i in range(n_alpha_bins):\n",
    "                    # stack_data index shifted by n_alpha_bins\n",
    "                    stack_data[n_alpha_bins + i].append(fracs_peak[i])\n",
    "                \n",
    "                # The boundary line is the total fraction of Low PI events\n",
    "                boundary_line.append(tot_low / total)\n",
    "\n",
    "        if energies_for_stackplot:\n",
    "            fig5, ax5 = plt.subplots(figsize=(12, 7))\n",
    "            \n",
    "            # We recycle the 'colors' list so both blocks look like full gradients.\n",
    "            full_colors = list(colors) + list(colors)\n",
    "            \n",
    "            # We only want 1 set of labels for the legend, so we use labels for first N, then None\n",
    "            full_labels = labels + [None] * n_alpha_bins\n",
    "            \n",
    "            ax5.stackplot(energies_for_stackplot, stack_data, labels=full_labels, colors=full_colors)\n",
    "            \n",
    "            # Plot the Boundary Line\n",
    "            ax5.plot(energies_for_stackplot, boundary_line, color='white', linewidth=3, linestyle='-')\n",
    "            ax5.plot(energies_for_stackplot, boundary_line, color='black', linewidth=1.5, linestyle='--')\n",
    "            \n",
    "            # Add Text Annotations to clarify regions using 'pe' for path effects\n",
    "            mid_energy = (min(energies) + max(energies)) / 2\n",
    "            ax5.text(7, 0.09, \"Tail PI\", ha='center', va='center',\n",
    "                     fontsize=12, fontweight='bold', color='white',  path_effects=[pe.withStroke(linewidth=3, foreground=\"black\")])\n",
    "            ax5.text(mid_energy, 0.9, \"Peak PI\", ha='center', va='center',\n",
    "                     fontsize=12, fontweight='bold', color='white', path_effects=[pe.withStroke(linewidth=3, foreground=\"black\")])\n",
    "\n",
    "            ax5.set(title=\"Alpha Fraction Distribution (Split by PI Selection)\",\n",
    "                    xlabel=\"Energy (keV)\",\n",
    "                    ylabel=\"Fraction of Total Events (stacked)\",\n",
    "                    ylim=(0, 1),\n",
    "                    xlim=(min(energies), max(energies)))\n",
    "            \n",
    "            # Legend (only shows the alpha bins once)\n",
    "            ax5.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), fontsize='small', title=\"Alpha Bins\")\n",
    "            fig5.tight_layout(rect=[0, 0, 0.8, 1])\n",
    "            pdf.savefig(fig5); plt.close(fig5)\n",
    "\n",
    "        print(\"   -> Generating μ vs. Energy plots for each alpha bin...\")\n",
    "        \n",
    "        erf_tail_params_log = []\n",
    "        erf_tail_params_log_low = []\n",
    "        \n",
    "        last_successful_ebreak_peak = None\n",
    "        last_successful_ebreak_low = None\n",
    "        \n",
    "        for i in range(n_alpha_bins):\n",
    "            alpha_low, alpha_high = alpha_bin_centers[i] - d_alpha/2.0, alpha_bin_centers[i] + d_alpha/2.0\n",
    "            mu_peak_slice = np.array([res['mu_bins'][i] for res in peak_results])\n",
    "            mu_err_peak_slice = np.array([res['mu_bins_err'][i] for res in peak_results])\n",
    "            mu_low_slice = np.array([res['mu_bins'][i] for res in low_results])\n",
    "            mu_err_low_slice = np.array([res['mu_bins_err'][i] for res in low_results])\n",
    "\n",
    "            fig_bin, ax_bin = plt.subplots()\n",
    "            ax_bin.errorbar(energies, mu_peak_slice, yerr=mu_err_peak_slice, c='deeppink', fmt='.', capsize=3, label=\"Peak PI\")\n",
    "            ax_bin.errorbar(energies, mu_low_slice, yerr=mu_err_low_slice, c='cornflowerblue', fmt='s', markersize=3, capsize=3, label=\"Tail PI\")\n",
    "            \n",
    "            e_fit_plot = np.linspace(energies.min(), energies.max(), 200)\n",
    "            erf_tail_params_this_bin = [np.nan] * 7\n",
    "            erf_tail_params_this_bin_low = [np.nan] * 7\n",
    "\n",
    "            # Fit Peak PI data\n",
    "            mask_peak = np.isfinite(mu_peak_slice) & (mu_err_peak_slice > 0)\n",
    "            if np.sum(mask_peak) > 6:\n",
    "                x_peak, y_peak, yerr_peak = energies[mask_peak], mu_peak_slice[mask_peak], mu_err_peak_slice[mask_peak]\n",
    "                try:\n",
    "                    p0_erf = [2 * alpha_high, 2, 6, -2.0, 0.1, 6.0, 2.0]\n",
    "                    \n",
    "                    lower_bounds = [2 * alpha_high - 0.001, 1.5, 5, -np.inf, 0, 4, 0.1]\n",
    "                    upper_bounds = [2 * alpha_high + 0.001, 6, np.inf, 0, 1, 12, 10]\n",
    "                    \n",
    "                    if last_successful_ebreak_peak is not None:\n",
    "                        lower_bounds[1] = last_successful_ebreak_peak\n",
    "                        p0_erf[1] = last_successful_ebreak_peak\n",
    "                    \n",
    "                    bounds_erf = (lower_bounds, upper_bounds)\n",
    "                    popt_erf, _ = curve_fit(peak_with_erf_tail, x_peak, y_peak, p0=p0_erf, sigma=yerr_peak, maxfev=10000, bounds=bounds_erf)\n",
    "                    \n",
    "                    ax_bin.plot(e_fit_plot, peak_with_erf_tail(e_fit_plot, *popt_erf), color='k', ls='-', lw=2)\n",
    "                    erf_tail_params_this_bin = popt_erf\n",
    "                    last_successful_ebreak_peak = popt_erf[1]\n",
    "                except (RuntimeError, ValueError):\n",
    "                    pass\n",
    "            \n",
    "            erf_tail_params_log.append(erf_tail_params_this_bin)\n",
    "\n",
    "            # Fit Low PI data\n",
    "            mask_low = np.isfinite(mu_low_slice) & (mu_err_low_slice > 0)\n",
    "            if np.sum(mask_low) > 4:\n",
    "                x_low, y_low, yerr_low = energies[mask_low], mu_low_slice[mask_low], mu_err_low_slice[mask_low]\n",
    "                try:\n",
    "                    p0_erf_low = [1, 3, 2.0, -2.0, 0.1, 6.0, 0.5]\n",
    "\n",
    "                    lower_bounds_low = [0, 2, 0, -np.inf, 0, 4, 0.1]\n",
    "                    upper_bounds_low = [2, 6, np.inf, 0, np.inf, 12, 1]\n",
    "                    \n",
    "                    if last_successful_ebreak_low is not None:\n",
    "                        lower_bounds_low[1] = last_successful_ebreak_low\n",
    "                        p0_erf_low[1] = last_successful_ebreak_low\n",
    "                    \n",
    "                    bounds_erf_low = (lower_bounds_low, upper_bounds_low)\n",
    "                    popt_erf_low, _ = curve_fit(peak_with_erf_tail, x_low, y_low, p0=p0_erf_low, sigma=yerr_low, maxfev=10000, bounds=bounds_erf_low)\n",
    "                    \n",
    "                    ax_bin.plot(e_fit_plot, peak_with_erf_tail(e_fit_plot, *popt_erf_low), color='k', ls='--', lw=1.5)\n",
    "                    erf_tail_params_this_bin_low = popt_erf_low\n",
    "                    last_successful_ebreak_low = popt_erf_low[1]\n",
    "                except (RuntimeError, ValueError):\n",
    "                    pass\n",
    "            \n",
    "            erf_tail_params_log_low.append(erf_tail_params_this_bin_low)\n",
    "\n",
    "            ax_bin.set(xlabel=\"Energy (keV)\", ylabel=r\"$\\mu$\", title=fr\"Modulation vs. Energy for ${alpha_low:.2f} \\leq \\alpha < {alpha_high:.2f}$\")\n",
    "            ax_bin.legend(); ax_bin.grid(True, linestyle='--', alpha=0.6); ax_bin.set_ylim(-0.15, 1.05)\n",
    "            pdf.savefig(fig_bin); plt.close(fig_bin)\n",
    "\n",
    "        print(\"   -> Generating parameter vs. alpha plots...\")\n",
    "        erf_param_names = ['Peak Amp (A_peak)', 'Break Energy (E_break)', 'Rise Index (α1)', 'Fall Index (α2)', 'Tail Height (C_tail)', 'Transition Energy (E_trans)', 'Transition Width (W_trans)']\n",
    "        \n",
    "        # Plot for Peak PI parameters\n",
    "        erf_params = np.array(erf_tail_params_log)\n",
    "        fig_erf, axes_erf = plt.subplots(4, 2, figsize=(12, 16), constrained_layout=True)\n",
    "        fig_erf.suptitle('Peak + Erf Tail Fit Parameters vs. α (Peak PI)', fontsize=16)\n",
    "        if len(erf_param_names) % 2 != 0: axes_erf.flat[-1].set_visible(False)\n",
    "        for idx, ax in enumerate(axes_erf.flat):\n",
    "            if idx < len(erf_param_names):\n",
    "                ax.plot(alpha_bin_centers, erf_params[:, idx], 'o-', color='deeppink')\n",
    "                ax.set(title=erf_param_names[idx], xlabel='α bin center', ylabel='Parameter Value'); ax.grid(True, ls=':')\n",
    "        pdf.savefig(fig_erf); plt.close(fig_erf)\n",
    "\n",
    "        # Plot for Low PI parameters\n",
    "        erf_params_low = np.array(erf_tail_params_log_low)\n",
    "        fig_erf_low, axes_erf_low = plt.subplots(4, 2, figsize=(12, 16), constrained_layout=True)\n",
    "        fig_erf_low.suptitle('Peak + Erf Tail Fit Parameters vs. α (Tail PI)', fontsize=16)\n",
    "        if len(erf_param_names) % 2 != 0: axes_erf_low.flat[-1].set_visible(False)\n",
    "        for idx, ax in enumerate(axes_erf_low.flat):\n",
    "            if idx < len(erf_param_names):\n",
    "                ax.plot(alpha_bin_centers, erf_params_low[:, idx], 'o-', color='cornflowerblue')\n",
    "                ax.set(title=erf_param_names[idx], xlabel='α bin center', ylabel='Parameter Value'); ax.grid(True, ls=':')\n",
    "        pdf.savefig(fig_erf_low); plt.close(fig_erf_low)\n",
    "\n",
    "    print(f\"Summary plots saved to '{filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3dd5b-1239-4eee-8c03-674dbfacfc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset selection and path setup\n",
    "dataset_config = {\n",
    "    \"original\": \"sim_data_mit\",\n",
    "    \"scrambled_80\": \"scrambled_sim_data_80percent\",\n",
    "    \"scrambled_50\": \"scrambled_sim_data_50percent\"\n",
    "}\n",
    "\n",
    "base_path = Path(os.path.expanduser(\"~/Library/CloudStorage/Box-Box/IXPE_rmfs\"))\n",
    "working_dir = Path(os.path.expanduser(\"~/Documents/IXPE\"))\n",
    "plot_output_dir = Path(\"/Users/leodrake/MIT Dropbox/Leonardo Drake/IXPE\")\n",
    "\n",
    "current_dataset_key = \"original\"\n",
    "\n",
    "# Resolve dataset paths and suffix\n",
    "try:\n",
    "    input_subdir = dataset_config[current_dataset_key]\n",
    "    root_dir = base_path / input_subdir\n",
    "    \n",
    "    output_suffix = '' if current_dataset_key == 'original' else f'-{current_dataset_key}'\n",
    "\n",
    "    print(f\"Processing dataset: '{current_dataset_key}'\")\n",
    "    print(f\"  - Input: {root_dir}\")\n",
    "    print(f\"  - Output Plots: {plot_output_dir}\")\n",
    "    print(f\"  - Suffix: '{output_suffix}'\")\n",
    "except KeyError:\n",
    "    raise KeyError(f\"Dataset key '{current_dataset_key}' not found.\")\n",
    "\n",
    "# Run configuration\n",
    "du_specifier = 'all'\n",
    "force_recompute = False\n",
    "generate_individual_plots = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5ca2d-06f6-458f-8863-a78c11e4e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing entry point\n",
    "if not root_dir.exists():\n",
    "    print(f\"Error: ROOT_DIR '{root_dir}' not found.\")\n",
    "else:\n",
    "    print(f\"--- Processing {du_specifier} ---\")\n",
    "    if force_recompute:\n",
    "        print(\"--- Caching is OFF (force_recompute = True) ---\")\n",
    "\n",
    "    try:\n",
    "        if not working_dir.exists():\n",
    "            working_dir.mkdir(parents=True, exist_ok=True)\n",
    "        os.chdir(working_dir)\n",
    "        print(f\"Working directory set to: {os.getcwd()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not set directory to {working_dir}: {e}\")\n",
    "        \n",
    "    if not plot_output_dir.exists():\n",
    "        try:\n",
    "            plot_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"Created plot output directory: {plot_output_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not create plot directory {plot_output_dir}: {e}\")\n",
    "\n",
    "    # Discover input files and group by energy\n",
    "    du_label, fits_files = list_sim_fits(du_specifier=du_specifier) \n",
    "\n",
    "    if not fits_files:\n",
    "        print(f\"No FITS files found for {du_label} in {root_dir}. Exiting.\")\n",
    "    else:\n",
    "        files_by_energy = defaultdict(list)\n",
    "        for fname in fits_files:\n",
    "            energy = parse_energy(fname)\n",
    "            if not np.isnan(energy):\n",
    "                files_by_energy[energy].append(fname)\n",
    "        \n",
    "        all_energies_found = sorted(files_by_energy.keys())\n",
    "        \n",
    "        energies_to_process = all_energies_found\n",
    "        \n",
    "        print(f\"Found {len(fits_files)} files. Processing {len(energies_to_process)} energy points (full range).\")\n",
    "\n",
    "        cache_dir = working_dir / f'NewRMFsADP/fit-cache-{du_label}{output_suffix}'\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        all_energies_for_du = []\n",
    "        all_peak_results_for_du = []\n",
    "        all_low_results_for_du = []\n",
    "\n",
    "        output_pdf_individual = plot_output_dir / f'process-all-sim-{du_label}{output_suffix}.pdf'\n",
    "        pdf = None\n",
    "        \n",
    "        if generate_individual_plots:\n",
    "            print(f\"Individual plots: {output_pdf_individual}\")\n",
    "            pdf = PdfPages(output_pdf_individual)\n",
    "\n",
    "        # Process each energy (cache or compute)\n",
    "        for e_kev_current_file in energies_to_process:\n",
    "            base_name = f\"sim-{e_kev_current_file*1000:05.0f}-{du_label}\"\n",
    "            cache_path_peak = cache_dir / f\"{base_name}-peak.npz\"\n",
    "            cache_path_low = cache_dir / f\"{base_name}-low.npz\"\n",
    "\n",
    "            try:\n",
    "                # Cache load\n",
    "                if cache_path_peak.exists() and cache_path_low.exists() and not force_recompute:\n",
    "                    print(f\"Loading {e_kev_current_file:.2f} keV from cache...\")\n",
    "                    \n",
    "                    with np.load(cache_path_peak, allow_pickle=True) as data:\n",
    "                        outputs_peak = {key: data[key].item() if data[key].ndim == 0 else data[key] for key in data}\n",
    "                    with np.load(cache_path_low, allow_pickle=True) as data:\n",
    "                        outputs_low = {key: data[key].item() if data[key].ndim == 0 else data[key] for key in data}\n",
    "                    \n",
    "                    all_energies_for_du.append(e_kev_current_file)\n",
    "                    all_peak_results_for_du.append(outputs_peak)\n",
    "                    all_low_results_for_du.append(outputs_low)\n",
    "                    continue\n",
    "\n",
    "                print(f\"Processing {e_kev_current_file:.2f} keV...\")\n",
    "                \n",
    "                # Compute per-energy outputs\n",
    "                current_files = files_by_energy[e_kev_current_file]\n",
    "                d = get_combined_data(current_files)\n",
    "                if d is None:\n",
    "                    print(f\"    Warning: No data loaded for {e_kev_current_file:.2f} keV.\")\n",
    "                    continue\n",
    "\n",
    "                pi_full, _, _, _ = extract_common_data(d, 1, 374, bgflag=False)\n",
    "                if len(pi_full) == 0:\n",
    "                    print(f\"    Warning: No data in full PI range for {e_kev_current_file:.2f} keV.\")\n",
    "                    continue\n",
    "\n",
    "                peak_lo, peak_hi = find_pi_peak_band(pi_full, bin_width=1, min_prominence=0.1)\n",
    "                non_lo, non_hi   = 1, peak_lo \n",
    "                print(f\"    PI Bands - Peak: {peak_lo:.1f}-{peak_hi:.1f}, Tail: {non_lo:.1f}-{non_hi:.1f}\")\n",
    "\n",
    "                base_title = f\"SIM {e_kev_current_file:.2f} keV ({du_label}) \"\n",
    "                title_non = f\"{base_title}– Tail PI {int(non_lo)}–{int(non_hi)}\"\n",
    "                title_pk = f\"{base_title}– PI {int(peak_lo)}–{int(peak_hi)}\"\n",
    "\n",
    "                outputs_peak, fit_figs_peak = fit_mu_alpha(d, peak_lo, peak_hi, nalpha=10, title=title_pk)\n",
    "                outputs_low, fit_figs_low = fit_mu_alpha(d, non_lo, non_hi, nalpha=10, title=title_non)\n",
    "\n",
    "                pi_low, _, alpha_low, _ = extract_common_data(d, non_lo, non_hi, bgflag=False)\n",
    "                n_tail_pi_bins = 4\n",
    "                low_pi_centers, low_pi_alpha_mean, low_pi_counts = summarize_alpha_vs_pi_bins(\n",
    "                    pi_low, alpha_low, non_lo, non_hi, n_bins=n_tail_pi_bins\n",
    "                )\n",
    "                outputs_low[\"alpha_pi_low_centers\"] = low_pi_centers\n",
    "                outputs_low[\"alpha_pi_low_mean\"] = low_pi_alpha_mean\n",
    "                outputs_low[\"alpha_pi_low_counts\"] = low_pi_counts\n",
    "\n",
    "                alpha_edges = np.linspace(0, 1.0, 21)\n",
    "                tail_pi_edges = np.linspace(non_lo, non_hi, n_tail_pi_bins + 1)\n",
    "                tail_hist = np.zeros((n_tail_pi_bins, len(alpha_edges) - 1), dtype=float)\n",
    "                tail_median = np.full(n_tail_pi_bins, np.nan)\n",
    "                tail_q1 = np.full(n_tail_pi_bins, np.nan)\n",
    "                tail_q3 = np.full(n_tail_pi_bins, np.nan)\n",
    "                for i in range(n_tail_pi_bins):\n",
    "                    sel = (pi_low >= tail_pi_edges[i]) & (pi_low < tail_pi_edges[i + 1])\n",
    "                    if np.any(sel):\n",
    "                        tail_hist[i], _ = np.histogram(alpha_low[sel], bins=alpha_edges)\n",
    "                        tail_median[i] = np.nanmedian(alpha_low[sel])\n",
    "                        tail_q1[i] = np.nanpercentile(alpha_low[sel], 25)\n",
    "                        tail_q3[i] = np.nanpercentile(alpha_low[sel], 75)\n",
    "\n",
    "                outputs_low[\"alpha_pi_tail_hist\"] = tail_hist\n",
    "                outputs_low[\"alpha_pi_tail_edges\"] = alpha_edges\n",
    "                outputs_low[\"alpha_pi_tail_pi_edges\"] = tail_pi_edges\n",
    "                outputs_low[\"alpha_pi_tail_median\"] = tail_median\n",
    "                outputs_low[\"alpha_pi_tail_q1\"] = tail_q1\n",
    "                outputs_low[\"alpha_pi_tail_q3\"] = tail_q3\n",
    "\n",
    "                all_energies_for_du.append(e_kev_current_file)\n",
    "                all_peak_results_for_du.append(outputs_peak)\n",
    "                all_low_results_for_du.append(outputs_low)\n",
    "                \n",
    "                # Cache outputs\n",
    "                np.savez_compressed(cache_path_peak, **outputs_peak)\n",
    "                np.savez_compressed(cache_path_low, **outputs_low)\n",
    "                print(f\"    Saved to cache.\")\n",
    "\n",
    "                # Per-energy plots\n",
    "                if pdf: \n",
    "                    figs_full = plot_pi_alpha(d, 1, 374, base_title + '- PI 1-374')\n",
    "                    for fig in figs_full or []: pdf.savefig(fig); plt.close(fig)\n",
    "\n",
    "                    figs_non = plot_pi_alpha(d, non_lo, non_hi, title_non, dist_component='Tail', a_color='cornflowerblue')\n",
    "                    if figs_non and len(figs_non) > 1: pdf.savefig(figs_non[1]);\n",
    "                    for fig in figs_non or []: plt.close(fig)\n",
    "\n",
    "                    figs_peak = plot_pi_alpha(d, peak_lo, peak_hi, title_pk, dist_component='Peak', a_color='deeppink')\n",
    "                    if figs_peak and len(figs_peak) > 1: pdf.savefig(figs_peak[1]);\n",
    "                    for fig in figs_peak or []: plt.close(fig)\n",
    "\n",
    "                    for fig in fit_figs_peak or []: pdf.savefig(fig); plt.close(fig)\n",
    "                    for fig in fit_figs_low or []: plt.close(fig)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR processing {e_kev_current_file:.2f} keV: {e}\")\n",
    "\n",
    "        if pdf:\n",
    "            pdf.close()\n",
    "            print(f\"Individual PDF generation complete.\")\n",
    "\n",
    "        # Summary PDF (2-8 keV)\n",
    "        if all_energies_for_du:\n",
    "            summary_mask = [(2.0 <= e <= 8.0) for e in all_energies_for_du]\n",
    "            if any(summary_mask):\n",
    "                summary_energies = [e for e, m in zip(all_energies_for_du, summary_mask) if m]\n",
    "                summary_peak_results = [r for r, m in zip(all_peak_results_for_du, summary_mask) if m]\n",
    "                summary_tail_results = [r for r, m in zip(all_low_results_for_du, summary_mask) if m]\n",
    "                summary_filename = plot_output_dir / f\"simulation-summary-plots-{du_label}{output_suffix}.pdf\"\n",
    "                generate_summary_sim_plots(summary_energies, summary_peak_results, summary_tail_results,\n",
    "                                           du_label, filename=str(summary_filename))\n",
    "            else:\n",
    "                print(\"No energies in 2–8 keV range. Skipping summary.\")\n",
    "        else:\n",
    "            print(f\"No valid data processed. Skipping summary.\")\n",
    "\n",
    "    print(f\"--- Processing complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ciao-4.17",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
