{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face45fe-27cf-418b-874e-ee2c4ea80f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from tqdm.auto import tqdm\n",
    "from multiprocess import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09694543-678b-40c2-b332-629b3b411c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Parameters (User Configuration) ---\n",
    "\n",
    "# The percentage of events to scramble (e.g., 80 for 80%)\n",
    "SCRAMBLE_PERCENT = 50\n",
    "\n",
    "# The base path where the input simulation data is stored\n",
    "# Note: Using an f-string for the Box path to make it clear\n",
    "user_home = os.path.expanduser(\"~\")\n",
    "BASE_INPUT_PATH = os.path.join(user_home, 'Library/CloudStorage/Box-Box/IXPE_rmfs/sim_data_mit')\n",
    "\n",
    "\n",
    "# The base path for the output scrambled data. A subdirectory will be created.\n",
    "BASE_OUTPUT_PATH = os.path.join(user_home, 'Library/CloudStorage/Box-Box/IXPE_rmfs')\n",
    "\n",
    "# The column to scramble. Choose 'DETPHI1' or 'DETPHI2' based on your analysis pipeline.\n",
    "# We'll use DETPHI2 as it's common in recent processing, but verify for your use case.\n",
    "TARGET_COLUMN = 'DETPHI2'\n",
    "\n",
    "# The FITS extension containing the event data\n",
    "EVENTS_EXTENSION = 'EVENTS'\n",
    "\n",
    "# --- Derived Parameters (Do not change) ---\n",
    "\n",
    "# Create the full output path including the percentage directory\n",
    "output_dir_name = f'scrambled_sim_data_{SCRAMBLE_PERCENT}percent'\n",
    "FULL_OUTPUT_PATH = os.path.join(BASE_OUTPUT_PATH, output_dir_name)\n",
    "\n",
    "# List of detector units\n",
    "detector_units = ['du1', 'du2', 'du3']\n",
    "\n",
    "# List of simulation number prefixes\n",
    "sim_numbers = range(1000, 10001, 100) # Goes from 1000 to 10000 in steps of 100\n",
    "\n",
    "print(f\"--- Configuration ---\")\n",
    "print(f\"Scrambling Percentage: {SCRAMBLE_PERCENT}%\")\n",
    "print(f\"Input Data Path:       {BASE_INPUT_PATH}\")\n",
    "print(f\"Output Data Path:      {FULL_OUTPUT_PATH}\")\n",
    "print(f\"Target Column:         {TARGET_COLUMN}\")\n",
    "print(\"--------------------\")\n",
    "\n",
    "# Create the main output directory if it doesn't exist\n",
    "os.makedirs(FULL_OUTPUT_PATH, exist_ok=True)\n",
    "print(f\"Output directory '{FULL_OUTPUT_PATH}' is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651f04d-8be8-44dc-811a-7f990faed546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scramble_fits_file_fast(input_path, output_path, scramble_percent, column_name):\n",
    "    \"\"\"\n",
    "    OPTIMIZED & FAST VERSION:\n",
    "    This function safely modifies the FITS data in-memory and writes to a\n",
    "    new file. It is much faster because it avoids rebuilding the entire FITS\n",
    "    table from scratch. The original file on disk is never altered.\n",
    "    \"\"\"\n",
    "    # Note: The EVENTS_EXTENSION variable is passed from the main script's scope\n",
    "    try:\n",
    "        with fits.open(input_path, mode='update') as hdul:\n",
    "            event_data = hdul[EVENTS_EXTENSION].data\n",
    "            n_events = len(event_data)\n",
    "\n",
    "            if n_events == 0:\n",
    "                hdul.writeto(output_path, overwrite=True)\n",
    "                return f\"Success: {os.path.basename(input_path)} (copied empty)\"\n",
    "\n",
    "            n_to_scramble = int(n_events * (scramble_percent / 100.0))\n",
    "            if n_to_scramble == 0:\n",
    "                hdul.writeto(output_path, overwrite=True)\n",
    "                return f\"Success: {os.path.basename(input_path)} (copied original)\"\n",
    "\n",
    "            scramble_indices = np.random.choice(n_events, size=n_to_scramble, replace=False)\n",
    "            random_phis = np.random.uniform(-np.pi, np.pi, n_to_scramble).astype(np.float32)\n",
    "\n",
    "            event_data[column_name][scramble_indices] = random_phis\n",
    "            \n",
    "            hdul.writeto(output_path, overwrite=True)\n",
    "            \n",
    "            return f\"Success: {os.path.basename(input_path)}\"\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return f\"ERROR: File not found at {input_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"ERROR processing {os.path.basename(input_path)}: {e}\"\n",
    "\n",
    "# --- MAIN PARALLEL PROCESSING LOOP ---\n",
    "\n",
    "# Step 1: Create a list of all tasks to be done.\n",
    "tasks = []\n",
    "print(\"Preparing list of files to process...\")\n",
    "for du in detector_units:\n",
    "    du_output_path = os.path.join(FULL_OUTPUT_PATH, du)\n",
    "    os.makedirs(du_output_path, exist_ok=True)\n",
    "    \n",
    "    for num in sim_numbers:\n",
    "        filename_base = f\"sim_{num:05d}_pol_recon\"\n",
    "        input_filename = f\"{filename_base}.fits\"\n",
    "        output_filename = f\"{filename_base}_scrambled_{SCRAMBLE_PERCENT}.fits\"\n",
    "        \n",
    "        input_filepath = os.path.join(BASE_INPUT_PATH, du, input_filename)\n",
    "        output_filepath = os.path.join(du_output_path, output_filename)\n",
    "        \n",
    "        tasks.append((input_filepath, output_filepath, SCRAMBLE_PERCENT, TARGET_COLUMN))\n",
    "\n",
    "# Step 2: Run the tasks in parallel.\n",
    "# This guard is still essential for safety, especially on macOS and Windows.\n",
    "if __name__ == '__main__':\n",
    "    print(f\"Starting parallel processing of {len(tasks)} files using dill backend...\")\n",
    "    \n",
    "    # The key change: Using Pool from the 'multiprocess' library\n",
    "    with Pool() as pool:\n",
    "        # pool.starmap works identically but can handle notebook functions\n",
    "        results = list(tqdm(pool.starmap(scramble_fits_file_fast, tasks), total=len(tasks)))\n",
    "\n",
    "    print(\"\\n--- Processing Complete ---\")\n",
    "    # Step 3: Report any errors.\n",
    "    errors = [res for res in results if res.startswith(\"ERROR\")]\n",
    "    if errors:\n",
    "        print(f\"\\nEncountered {len(errors)} errors during processing:\")\n",
    "        for err in errors:\n",
    "            print(f\"- {err}\")\n",
    "    else:\n",
    "        print(\"\\n✅ All files processed successfully with no errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d818a-84d7-42df-a7a3-ed60213fc198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ===================================================================\n",
    "# --- 1. CONFIGURATION ---\n",
    "# ===================================================================\n",
    "\n",
    "# The number of photons to randomly sample from EACH energy file.\n",
    "# A larger number gives better statistics but results in a larger final file.\n",
    "# 5000 is a reasonable starting point.\n",
    "N_PHOTONS_PER_FILE = 5000\n",
    "\n",
    "# The base path where your data folders are located\n",
    "BASE_PATH = os.path.join(os.path.expanduser(\"~\"), 'Library/CloudStorage/Box-Box/IXPE_rmfs')\n",
    "\n",
    "# The specific input directory for the scrambled data\n",
    "INPUT_DIR = \"scrambled_sim_data_80percent\"\n",
    "\n",
    "# Where to save the final combined FITS files\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"combined_events_from_scrambled\")\n",
    "\n",
    "# Define the energy ranges (in simulation units, e.g., 1000 for 1.0 keV)\n",
    "# for each detector unit. The upper bound is exclusive.\n",
    "ENERGY_RANGES = {\n",
    "    'du1': range(1000, 10001, 100), # 1.0 to 10.0 keV\n",
    "    'du2': range(1000, 10001, 100), # 1.0 to 10.0 keV\n",
    "    'du3': range(1000, 5001, 100),  # 1.0 to 5.0 keV\n",
    "}\n",
    "\n",
    "# The FITS extension containing the event data\n",
    "EVENTS_EXTENSION = 'EVENTS'\n",
    "\n",
    "# --- End of Configuration ---\n",
    "\n",
    "# Automatically create the output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\" Configuration set. Final files will be saved in: {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# --- 2. MAIN PROCESSING LOOP ---\n",
    "# ===================================================================\n",
    "\n",
    "# Loop over each detector unit defined in the energy ranges\n",
    "for du_name in ENERGY_RANGES.keys():\n",
    "    print(f\"\\n--- Processing {du_name} ---\")\n",
    "    \n",
    "    # List to hold the sampled data from all files for this DU\n",
    "    all_sampled_events = []\n",
    "    template_header = None # To store the header from the first file\n",
    "\n",
    "    # Construct the path to the current DU's data\n",
    "    du_path = os.path.join(BASE_PATH, INPUT_DIR, du_name)\n",
    "    if not os.path.isdir(du_path):\n",
    "        print(f\" Warning: Directory not found, skipping: {du_path}\")\n",
    "        continue\n",
    "\n",
    "    # Loop through each energy file for the current DU\n",
    "    energy_range = ENERGY_RANGES[du_name]\n",
    "    for sim_num in tqdm(energy_range, desc=f\"Sampling {du_name} files\"):\n",
    "        \n",
    "        # Construct the expected filename\n",
    "        filename = f\"sim_{sim_num:05d}_pol_recon_scrambled_80.fits\"\n",
    "        filepath = os.path.join(du_path, filename)\n",
    "\n",
    "        if not os.path.exists(filepath):\n",
    "            continue # Skip if a file for this energy doesn't exist\n",
    "\n",
    "        try:\n",
    "            # Open the FITS file\n",
    "            with fits.open(filepath) as hdul:\n",
    "                event_data = hdul[EVENTS_EXTENSION].data\n",
    "                \n",
    "                # If this is the first valid file, grab its header as a template\n",
    "                if template_header is None:\n",
    "                    template_header = hdul[EVENTS_EXTENSION].header\n",
    "\n",
    "                # Determine how many events to sample\n",
    "                n_events_in_file = len(event_data)\n",
    "                if n_events_in_file == 0:\n",
    "                    continue # Skip empty files\n",
    "                \n",
    "                n_to_sample = min(n_events_in_file, N_PHOTONS_PER_FILE)\n",
    "                \n",
    "                # Randomly select row indices without replacement\n",
    "                sample_indices = np.random.choice(n_events_in_file, size=n_to_sample, replace=False)\n",
    "                \n",
    "                # Append the sampled event rows to our master list\n",
    "                all_sampled_events.append(event_data[sample_indices])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error processing file {filename}: {e}\")\n",
    "\n",
    "    # After checking all files for the DU, combine the sampled events\n",
    "    if not all_sampled_events:\n",
    "        print(f\" No events were sampled for {du_name}. No output file will be created.\")\n",
    "        continue\n",
    "\n",
    "    # Concatenate all the data chunks into a single large table\n",
    "    print(f\"Concatenating sampled events for {du_name}...\")\n",
    "    final_event_table = np.concatenate(all_sampled_events)\n",
    "    total_events = len(final_event_table)\n",
    "    print(f\"Total events in combined file for {du_name}: {total_events:,}\")\n",
    "\n",
    "    # Create a new FITS file for the combined data\n",
    "    primary_hdu = fits.PrimaryHDU() # A minimal primary HDU\n",
    "    events_hdu = fits.BinTableHDU(data=final_event_table, header=template_header)\n",
    "    \n",
    "    hdul_out = fits.HDUList([primary_hdu, events_hdu])\n",
    "    \n",
    "    # Define the output path and save the file\n",
    "    output_filename = f\"{du_name}_combined_sampled_events.fits\"\n",
    "    output_filepath = os.path.join(OUTPUT_DIR, output_filename)\n",
    "    \n",
    "    hdul_out.writeto(output_filepath, overwrite=True)\n",
    "    hdul_out.close()\n",
    "    \n",
    "    print(f\" Successfully created: {output_filepath}\")\n",
    "\n",
    "print(\"\\n--- All processing complete! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e730acc-bdf2-4bd4-82a1-1f621cc634f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# ===================================================================\n",
    "# --- 3. ADD PI COLUMN TO COMBINED FILES ---\n",
    "# ===================================================================\n",
    "\n",
    "# This script uses the OUTPUT_DIR and ENERGY_RANGES defined in the previous cells.\n",
    "print(f\"Adding 'PI' column to combined files in: {OUTPUT_DIR}\")\n",
    "\n",
    "# Conversion factor from PHA (ADC counts) to PI channels.\n",
    "# PI = PHA / (3000 ADC/keV * 0.04 keV/PI_channel) = PHA / 120\n",
    "pha2pi = 1.0 / (3000.0 * 0.04)\n",
    "\n",
    "# Loop over each detector unit we've processed\n",
    "for du_name in tqdm(ENERGY_RANGES.keys(), desc=\"Processing DUs\"):\n",
    "    \n",
    "    # Construct the path to the combined file for this DU\n",
    "    combined_filename = f\"{du_name}_combined_sampled_events.fits\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, combined_filename)\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"⚠️ Warning: File not found, skipping: {filepath}\")\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        # Open the FITS file in 'update' mode to allow for changes\n",
    "        with fits.open(filepath, mode='update') as hdul:\n",
    "            \n",
    "            # --- 1. Perform Calculation ---\n",
    "            events_hdu = hdul[EVENTS_EXTENSION]\n",
    "            pha_data = events_hdu.data['PHA'].astype(float)\n",
    "            pi_data = pha_data * pha2pi\n",
    "            \n",
    "            # --- 2. Add the New Column ---\n",
    "            # Get the original table columns\n",
    "            original_cols = events_hdu.columns\n",
    "            \n",
    "            # Create a new FITS Column object for our PI data\n",
    "            # Format 'E' is for a standard 32-bit floating-point number\n",
    "            pi_col = fits.Column(name='PI', format='E', array=pi_data)\n",
    "            \n",
    "            # Create a new column definition object by adding our new column\n",
    "            new_cols = original_cols + pi_col\n",
    "            \n",
    "            # --- 3. Replace the Old Table with the New One ---\n",
    "            # Create a new table HDU from the new columns, preserving the original header\n",
    "            new_hdu = fits.BinTableHDU.from_columns(new_cols, header=events_hdu.header)\n",
    "            \n",
    "            # Replace the old EVENTS HDU in the file with our new one\n",
    "            hdul[EVENTS_EXTENSION] = new_hdu\n",
    "            \n",
    "            # The changes will be saved automatically when the 'with' block closes.\n",
    "            print(f\"✅ Successfully added 'PI' column to {combined_filename}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR processing {combined_filename}: {e}\")\n",
    "\n",
    "print(\"\\n--- PI column processing complete! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5fb6ae-7262-4091-8dd9-2c87b92ab89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Update with the path to your combined event file\n",
    "INPUT_FITS_FILE = '/Users/leodrake/Library/CloudStorage/Box-Box/IXPE_rmfs/combined_events_from_scrambled/du1_combined_sampled_events.fits' \n",
    "OUTPUT_FITS_FILE = '/Users/leodrake/Library/CloudStorage/Box-Box/IXPE_rmfs/combined_events_from_scrambled/du1_streamlined_events.fits'\n",
    "\n",
    "# IXPE detector pixel size in mm\n",
    "MM_PER_PIXEL = 0.050\n",
    "\n",
    "# The FITS extension containing the event data\n",
    "EVENTS_EXTENSION = 'EVENTS'\n",
    "\n",
    "print(f\"Reading input file: {INPUT_FITS_FILE}\")\n",
    "\n",
    "# --- 2. Read Source Data ---\n",
    "try:\n",
    "    with fits.open(INPUT_FITS_FILE) as hdul:\n",
    "        source_data = hdul[EVENTS_EXTENSION].data\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Input file not found at '{INPUT_FITS_FILE}'. Please check the path.\")\n",
    "    # Exit gracefully if in a script, or just stop if in a notebook\n",
    "    # import sys; sys.exit()\n",
    "except (KeyError, IndexError):\n",
    "    print(f\"❌ ERROR: Could not find the '{EVENTS_EXTENSION}' extension in the FITS file.\")\n",
    "    # import sys; sys.exit()\n",
    "else:\n",
    "    # --- 3. Create New Column Data from Source ---\n",
    "    print(\"Processing columns...\")\n",
    "\n",
    "    # Direct copies\n",
    "    trg_id_new = source_data['TRG_ID']\n",
    "    time_new = source_data['TIME']\n",
    "    status_new = source_data['STATUS']\n",
    "    status2_new = source_data['STATUS2']\n",
    "    \n",
    "    # Copy and typecast PI to integer\n",
    "    pi_new = source_data['PI'].astype('int32')\n",
    "\n",
    "    # Calculate W_MOM, handling potential division by zero\n",
    "    m2l = source_data['TRK_M2L']\n",
    "    m2t = source_data['TRK_M2T']\n",
    "    denominator = m2l + m2t\n",
    "    # Use np.divide to safely handle division by zero, filling with 0.0\n",
    "    w_mom_new = np.divide(m2l - m2t, denominator, out=np.zeros_like(denominator, dtype=float), where=denominator!=0)\n",
    "\n",
    "    # Calculate POS(X,Y) in pixels and stack into a 2D array\n",
    "    pos_x_new = source_data['ABSX'] / MM_PER_PIXEL\n",
    "    pos_y_new = source_data['ABSY'] / MM_PER_PIXEL\n",
    "    pos_new = np.column_stack((pos_x_new, pos_y_new))\n",
    "\n",
    "    # --- 4. Define the Columns for the New FITS Table ---\n",
    "    # This list defines the structure of the new file.\n",
    "    # FITS format codes: J=Int32, D=Real8, 2X=2-element Bit Array, E=Real4, 2E=2-element Real4 vector\n",
    "    new_columns = [\n",
    "        fits.Column(name='TRG_ID',    format='J',  array=trg_id_new),\n",
    "        fits.Column(name='TIME',      format='D',  unit='s', array=time_new),\n",
    "        fits.Column(name='STATUS',    format='2X', array=status_new),\n",
    "        fits.Column(name='STATUS2',   format='2X', array=status2_new),\n",
    "        fits.Column(name='PI',        format='J',  unit='chan', array=pi_new),\n",
    "        fits.Column(name='W_MOM',     format='E',  array=w_mom_new),\n",
    "        fits.Column(name='POS',       format='2E', unit='pixel', array=pos_new)\n",
    "    ]\n",
    "    \n",
    "    # --- 5. Create and Save the New FITS File ---\n",
    "    # Create the new binary table HDU from our defined columns\n",
    "    new_table_hdu = fits.BinTableHDU.from_columns(new_columns, name='EVENTS')\n",
    "\n",
    "    # Create a minimal primary HDU\n",
    "    primary_hdu = fits.PrimaryHDU()\n",
    "    \n",
    "    # Combine into a final HDU list\n",
    "    final_hdul = fits.HDUList([primary_hdu, new_table_hdu])\n",
    "    \n",
    "    # Write to disk, overwriting if the file already exists\n",
    "    final_hdul.writeto(OUTPUT_FITS_FILE, overwrite=True)\n",
    "    final_hdul.close()\n",
    "\n",
    "    print(f\"\\n✨ Successfully created streamlined FITS file: {OUTPUT_FITS_FILE}\")\n",
    "    print(f\"   - Total Events: {len(trg_id_new):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e54d2-b9ac-4363-85a7-7b28f14f7974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (heasoft-6.35.1)",
   "language": "python",
   "name": "heasoft-6.35.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
